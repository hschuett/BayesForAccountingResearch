{
  "articles": [
    {
      "path": "01-classic-vs-bayes.html",
      "title": "Simulation: Classical vs. Bayesian Regression",
      "description": "Code for the Section 2: Bayesian inference compared to classical inference\n",
      "author": [
        {
          "name": "Harm Schuett",
          "url": "https://hschuett.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\n1. Introduction and preliminaries\r\n2. Creating the simulated data\r\n3. OLS regressions\r\n3.1. OLS fits of all 50 samples\r\n3.2. OlS estimates of the first sample\r\n\r\n4. Fig. 1: Classical hypothesis tests\r\n4.1. Fig.1, Panel A\r\n4.2. Fig.1, Panel B\r\n\r\n5. Additional t-test figure\r\n6. Fitting the Bayesian models\r\n6.1 Bayesian model with diffuse/very weak priors\r\n6.2. Bayesian model with weakly informative priors\r\n\r\n7. Fig. 2: Visualization of Bayesian inference\r\n7.1. Data preparations\r\n7.2. Fig.2, Subfigure 1\r\n7.3. Fig.2, Subfigure 2\r\n\r\n8. Fig. 3: Priors vs Likelihood\r\n8.1. Generate a larger sample\r\n8.2. Fitting the weakly informative model to larger sample\r\n8.3. Collecting Prior, likelihood, posterior for small, current sample\r\n8.4. Collecting Prior, likelihood, posterior for larger sample\r\n8.5. Fig. 3, Panel A and Panel B\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(broom)\r\nlibrary(patchwork)\r\nlibrary(cmdstanr)\r\nsource(\"../R/00-utils.R\")\r\nkable <- knitr::kable\r\ntheme_set(theme_prodgray())\r\n\r\n\r\n\r\n1. Introduction and preliminaries\r\nThis markdown file contains all the code necessary to replicate the figures, models and results used in Section 2: Bayesian inference compared to classical inference of the Paper What Can Bayesian Inference Do for Accounting Research?. All the code can also be found in the repo. It contains 00-utils.R which contains a few helper functions for graphs and tables.\r\nNote: I started using the newer cmdstanr package instead of the older rstan package because it likely is the future of the R based stan ecosystem. I also really like its api, which is very close to the api of the pystan package. An additional advantage (I hope) is thus that most model fitting code should be more or less directly transferable to pystan for those that want to work in python.\r\nInstalling cmdstanr used to be tricky at times because one needs a working c++ toolchain. But it is much smoother now. Please see the cmdstanr doc for installation instructions\r\n2. Creating the simulated data\r\nFirst we create the 50 samples of 50 observations each\r\n\r\n\r\nset.seed(888)\r\nn_samples <- 50\r\nn_obs <- 50\r\nx_fix <- rnorm(n = n_obs, 0, 1)\r\ngen_data <- function(n) tibble(u = rnorm(n, 0, 20), x = x_fix, y = 1 + 2 * x_fix + u)\r\n\r\n# gen samples\r\nsamples <- tibble(id = 1:n_samples)\r\nsamples$data <- map(rep.int(n_obs, n_samples), gen_data)\r\n\r\n\r\n\r\nNext we create the relevant figures\r\n3. OLS regressions\r\n3.1. OLS fits of all 50 samples\r\n\r\n\r\n# gen ols estimates\r\nsamples$ols_fit <- map(samples$data, function(dat) tidy(lm(y ~ x, data = dat)))\r\nols_estimates <- samples %>%\r\n  unnest(ols_fit) %>%\r\n  mutate(param = if_else(term == \"x\", \"a1\", \"a0\")) %>%\r\n  select(id, param, estimate) %>%\r\n  spread(key = param, value = estimate) %>%\r\n  mutate(\r\n    plabs = paste0(\"(\", round(a0, 1), \",\", round(a1, 1), \")\"),\r\n    first_sample = as.factor(c(1, rep.int(0, times = n_samples - 1)))\r\n  )\r\n\r\n\r\n\r\n3.2. OlS estimates of the first sample\r\n\r\n\r\nkable(samples$ols_fit[[1]])\r\n\r\n\r\nterm\r\nestimate\r\nstd.error\r\nstatistic\r\np.value\r\n(Intercept)\r\n0.7602868\r\n2.873058\r\n0.2646264\r\n0.7924303\r\nx\r\n4.3777130\r\n2.653451\r\n1.6498189\r\n0.1055098\r\n\r\n4. Fig. 1: Classical hypothesis tests\r\nWe need the following variables for scaling Fig. 1\r\n\r\n\r\nrange_x <- range(ols_estimates$a0)\r\nrange_y <- range(ols_estimates$a1)\r\nnudge_x <- 0.1\r\nsample_a0 <- ols_estimates$a0[1]\r\nsample_a1 <- ols_estimates$a1[1]\r\nr <- sd(ols_estimates$a1)\r\n\r\n\r\n\r\n4.1. Fig.1, Panel A\r\n\r\n\r\nf1.panA <-\r\n  ols_estimates %>%\r\n  ggplot(aes(x = a0, y = a1, shape = first_sample, color = first_sample)) +\r\n  geom_point(size = 2) +\r\n  xlim(range_x) +\r\n  ylim(range_y) +\r\n  scale_color_manual(values = c(\"gray80\", \"black\")) +\r\n  labs(\r\n    x = expression(\" Intercept estimate \" ~ hat(a)[0]),\r\n    y = expression(\" Slope estimate \" ~ hat(a)[1]),\r\n    subtitle = \"Sampling variation\"\r\n  ) +\r\n  geom_segment(aes(color = first_sample), xend = 1, yend = 2, linetype = 2, alpha = 0.5) +\r\n  annotate(\"point\", x = 1, y = 2, color = \"black\", size = 2, shape = 15) +\r\n  annotate(\"text\", x = 1.4, y = 1.9, label = \"Truth (1, 2)\", hjust = 0, color = \"black\", size = 3) +\r\n  annotate(\"text\",\r\n    x = sample_a0 + 0.4,\r\n    y = sample_a1 + -0.1,\r\n    label = paste0(\"Sample (\", round(sample_a0, 1), \", \", round(sample_a1, 1), \")\"),\r\n    hjust = 0, color = \"black\", size = 3\r\n  ) +\r\n  annotate(\"point\", x = 0, y = 0, color = \"black\", size = 2, shape = 15) +\r\n  annotate(\"text\", x = 0.4, y = -0.1, label = \"H0 (0, 0)\", hjust = 0, color = \"black\", size = 3) +\r\n  theme(legend.position = \"none\", aspect.ratio=1)\r\n\r\n\r\n\r\n4.2. Fig.1, Panel B\r\n\r\n\r\nf1.panB <- \r\n  ols_estimates %>%\r\n  ggplot(aes(x = a0, y = a1, shape = first_sample, color = first_sample)) +\r\n  geom_point(size = 2) +\r\n  xlim(range_x) +\r\n  ylim(range_y) +\r\n  scale_color_manual(values = c(\"gray80\", \"black\")) +\r\n  labs(\r\n    x = expression(\" Intercept estimate \" ~ hat(a)[0]),\r\n    y = expression(\" Slope estimate \" ~ hat(a)[1]),\r\n    subtitle = \"Distance from hypothesis\"\r\n  ) +\r\n  theme(legend.position = \"none\") +\r\n  geom_segment(aes(color = first_sample), xend = 0, yend = 0, linetype = 2, alpha = 0.5) +\r\n  annotate(\"point\", x = 0, y = 0, color = \"black\", size = 2, shape = 15) +\r\n  annotate(\"text\", x = 0.4, y = -0.1, label = \"H0 (0, 0)\", hjust = 0, color = \"black\", size = 3) +\r\n  annotate(\"text\",\r\n    x = sample_a0 + 0.4,\r\n    y = sample_a1 + -0.1,\r\n    label = paste0(\"Sample (\", round(sample_a0, 1), \", \", round(sample_a1, 1), \")\"),\r\n    hjust = 0, color = \"black\", size = 3\r\n  ) +\r\n  geom_hline(yintercept = c(r, -r), color = \"black\") +\r\n  geom_segment(\r\n    x = -4, xend = -4, y = -r, yend = r, color = \"black\",\r\n    arrow = arrow(length = unit(0.07, \"inches\"), ends = \"both\", type = \"closed\")\r\n  ) +\r\n  annotate(\"text\",\r\n    x = -3.4, y = -2.2,\r\n    size = 3, hjust = 0, vjust = 0, color = \"black\",\r\n    label = \"One standard deviation\"\r\n  ) +\r\n  theme(legend.position = \"none\", aspect.ratio=1)\r\n\r\n\r\n\r\nSaving figure\r\n\r\n\r\nfig1 <- f1.panA + f1.panB + plot_annotation(tag_levels = \"A\")\r\nsave_fig(fig1, figname = \"fig1\", w = 6.2, h = 3.3)\r\n\r\n\r\n\r\n\r\n\r\nfig1\r\n\r\n\r\n\r\n\r\n5. Additional t-test figure\r\nThis one is not in the paper. I used it for an internal Ph.D. seminar\r\n\r\n\r\nt_dist <- data.frame(ps = rt(n=10000, df=n_obs-2))\r\ntest_stat <- 1.65\r\nquant_test <- round(1 - pt(test_stat, df=n_obs-2), 2)\r\n\r\nfZ <- \r\n  ggplot(data=t_dist, aes(x=ps)) +\r\n  geom_histogram(bins=50, color=\"white\", fill=\"gray30\") +\r\n  geom_segment(color=\"red\",\r\n               x=test_stat, y=0,\r\n               xend=test_stat, yend=200) +\r\n  geom_segment(color=\"red\",\r\n               x=-test_stat, y=0,\r\n               xend=-test_stat, yend=200) +\r\n  # geom_area(color=high_red, fill=high_red, alpha=0.5) +\r\n  annotate(\"text\", x=test_stat, y=200,\r\n           size=3, hjust=0, vjust=0,\r\n           label=paste0(\"P(t >= \", test_stat, \" | H0) = \", quant_test)) +\r\n  annotate(\"text\", x=-test_stat, y=200,\r\n           size=3, hjust=1, vjust=0,\r\n           label=paste0(\"P(t <= -\", test_stat, \" | H0) = \", quant_test)) +\r\n  scale_y_continuous(expand=c(0,0)) +\r\n  labs(x = expression((hat(a)-H0)/SE),\r\n       y = NULL,\r\n       subtitle = \"Occurrence in repeated sampling\") +\r\n  theme(legend.position = \"none\")\r\nfZ\r\n\r\n\r\n\r\n\r\n6. Fitting the Bayesian models\r\nTo fit a Bayesian model I use Stan, or, more precisely, its R bindings in cmdstanr. So, to fit a Bayesian model, we first need to:\r\nWrite the corresponding model in Stan code\r\nCompile the code into an model executable (an .exe file)\r\nMake a list of data to feed into the .exe file\r\nLet the model actually run and generate MCMC chains (or whatever algorithm is specified)\r\n6.1 Bayesian model with diffuse/very weak priors\r\n\r\n\r\nsample1 <- samples$data[[1]]\r\n\r\n\r\n\r\nThe following descriptive are useful when assessing what priors to use:\r\n\r\n\r\nsample1_descs <- c(\r\n  u_y = mean(sample1$y),\r\n  u_x = mean(sample1$x),\r\n  s_y = sd(sample1$y),\r\n  s_x = sd(sample1$x)\r\n) \r\n\r\nround(sample1_descs, 3)\r\n\r\n\r\n   u_y    u_x    s_y    s_x \r\n 0.499 -0.060 20.638  1.092 \r\n\r\nThe model itself is coded in the stan language. There are many excellent tutorials on Stan available online. So I won’t waste space explaining it here. For various reasons (e.g., debugging) it is customary to put the model code in a separate .stan file. All the model files can be found in the /Stan/ folder of the repo.\r\n\r\n\r\ncat(read_lines(\"../Stan/sim-vweak-priors.stan\"), sep = \"\\n\")\r\n\r\n\r\ndata{\r\n  int<lower=1> N;\r\n  real y[N];\r\n  real x[N];\r\n}\r\nparameters{\r\n  real a0;\r\n  real a1;\r\n  real<lower=0> sigma;\r\n}\r\nmodel{\r\n  vector[N] mu;\r\n  sigma ~ exponential( 1.0 / 21.0 );\r\n  a0 ~ normal( 0 , 100 );\r\n  a1 ~ normal( 0 , 100 );\r\n  for ( i in 1:N ) {\r\n    mu[i] = a0 + a1 * x[i];\r\n  }\r\n  y ~ normal( mu , sigma );\r\n}\r\n\r\nThe model code in stan language\r\nNext, we compile the model to an .exe file\r\n\r\n\r\nmodel_vweak_priors <- cmdstan_model(\"../Stan/sim-vweak-priors.stan\")\r\n\r\n\r\n\r\nNow, we prepare the list of data to feed into the model.\r\n\r\n\r\ninput_data <- list(\r\n  y = sample1$y,\r\n  x = sample1$x,\r\n  N = nrow(sample1)\r\n)\r\n\r\n\r\n\r\nNote how the list matches the names of the variables declared in the model’s data block\r\nWe run the model\r\n\r\n\r\nfit_vweak_priors <- model_vweak_priors$sample(\r\n  data = input_data,\r\n  iter_sampling = 1000,\r\n  iter_warmup = 1000,\r\n  chains = 4,\r\n  parallel_chains = 4,\r\n  seed = 1234,\r\n  refresh = 1000\r\n)\r\n\r\n\r\nRunning MCMC with 4 parallel chains...\r\n\r\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 1 finished in 0.1 seconds.\r\nChain 2 finished in 0.1 seconds.\r\nChain 3 finished in 0.1 seconds.\r\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 4 finished in 0.1 seconds.\r\n\r\nAll 4 chains finished successfully.\r\nMean chain execution time: 0.1 seconds.\r\nTotal execution time: 0.3 seconds.\r\n\r\nHere is the summary of the resulting posterior distribution of the model parameters\r\n\r\n\r\nfit_vweak_priors$summary(variables = c(\"sigma\", \"a0\", \"a1\")) |> \r\n  mutate(across(where(is.numeric), round, 3)) |> \r\n  kable()\r\n\r\n\r\nvariable\r\nmean\r\nmedian\r\nsd\r\nmad\r\nq5\r\nq95\r\nrhat\r\ness_bulk\r\ness_tail\r\nsigma\r\n20.556\r\n20.380\r\n2.125\r\n2.074\r\n17.446\r\n24.220\r\n1.003\r\n3094.064\r\n2515.482\r\na0\r\n0.756\r\n0.778\r\n2.921\r\n2.932\r\n-3.884\r\n5.581\r\n1.000\r\n3366.030\r\n2861.077\r\na1\r\n4.241\r\n4.241\r\n2.665\r\n2.681\r\n-0.186\r\n8.541\r\n1.000\r\n3496.024\r\n2752.886\r\n\r\nPosterior descriptives\r\nThis is one form of getting the mcmc chain output. We need the draws to draw inferences about the posterior.\r\n\r\n\r\npostdraws_vweak_prior <- fit_vweak_priors$draws(format = \"draws_df\")\r\nhead(postdraws_vweak_prior)\r\n\r\n\r\n# A draws_df: 6 iterations, 1 chains, and 4 variables\r\n  lp__     a0   a1 sigma\r\n1 -173  2.697  4.0    21\r\n2 -174 -1.066  6.8    18\r\n3 -173  0.071  5.6    19\r\n4 -174  2.678  6.5    23\r\n5 -174 -1.515  2.1    18\r\n6 -176 -1.275 -2.1    23\r\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\r\n\r\nThe posterior 90% centered credible interval:\r\n\r\n\r\nquantile(postdraws_vweak_prior$a1, probs = c(0.05, 0.95))\r\n\r\n\r\n        5%        95% \r\n-0.1861673  8.5411815 \r\n\r\nThe posterior probability that a1 < 0:\r\n\r\n\r\nemp_a1 <- ecdf(postdraws_vweak_prior$a1)\r\nemp_a1(0)\r\n\r\n\r\n[1] 0.05825\r\n\r\n6.2. Bayesian model with weakly informative priors\r\nThis is basically the same model, just with different hard-coded priors.\r\n\r\n\r\ncat(read_lines(\"../Stan/sim-wkinfo-priors.stan\"), sep = \"\\n\")\r\n\r\n\r\ndata{\r\n  int<lower=1> N;\r\n  real y[N];\r\n  real x[N];\r\n}\r\nparameters{\r\n  real a0;\r\n  real a1;\r\n  real<lower=0> sigma;\r\n}\r\nmodel{\r\n  vector[N] mu;\r\n  sigma ~ exponential( 1.0 / 21.0 );\r\n  a0 ~ normal( 0 , 100 );\r\n  a1 ~ normal( 0 , 4 );\r\n  for ( i in 1:N ) {\r\n    mu[i] = a0 + a1 * x[i];\r\n  }\r\n  y ~ normal( mu , sigma );\r\n}\r\n\r\n\r\n\r\nmodel_wkinfo_priors <- cmdstan_model(\"../Stan/sim-wkinfo-priors.stan\")\r\n\r\n\r\n\r\nBecause we use the same data (The list input_data), we can fit the new model now:\r\n\r\n\r\nfit_wkinfo_priors <- model_wkinfo_priors$sample(\r\n  data = input_data,\r\n  iter_sampling = 1000,\r\n  iter_warmup = 1000,\r\n  chains = 4,\r\n  parallel_chains = 4,\r\n  seed = 1234,\r\n  refresh = 1000\r\n)\r\n\r\n\r\nRunning MCMC with 4 parallel chains...\r\n\r\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 1 finished in 0.1 seconds.\r\nChain 2 finished in 0.1 seconds.\r\nChain 3 finished in 0.1 seconds.\r\nChain 4 finished in 0.1 seconds.\r\n\r\nAll 4 chains finished successfully.\r\nMean chain execution time: 0.1 seconds.\r\nTotal execution time: 0.3 seconds.\r\n\r\nHere is the summary of the resulting posterior distribution of the model parameters\r\n\r\n\r\nfit_wkinfo_priors$summary(variables = c(\"sigma\", \"a0\", \"a1\")) |> \r\n  mutate(across(where(is.numeric), round, 3)) |> \r\n  kable()\r\n\r\n\r\nvariable\r\nmean\r\nmedian\r\nsd\r\nmad\r\nq5\r\nq95\r\nrhat\r\ness_bulk\r\ness_tail\r\nsigma\r\n20.567\r\n20.358\r\n2.161\r\n2.072\r\n17.333\r\n24.506\r\n1.001\r\n4276.167\r\n3167.417\r\na0\r\n0.672\r\n0.690\r\n2.878\r\n2.897\r\n-4.040\r\n5.364\r\n1.000\r\n4199.116\r\n2467.601\r\na1\r\n2.978\r\n3.013\r\n2.213\r\n2.186\r\n-0.793\r\n6.560\r\n1.001\r\n4074.289\r\n3219.891\r\n\r\nPosterior descriptives\r\nThe posterior 90% centered credible interval:\r\n\r\n\r\npostdraws_wkinfo_prior <- fit_vweak_priors$draws(format = \"draws_df\")\r\nquantile(postdraws_wkinfo_prior$a1, probs = c(0.05, 0.95))\r\n\r\n\r\n        5%        95% \r\n-0.1861673  8.5411815 \r\n\r\nThe posterior probability that a1 < 0:\r\n\r\n\r\nemp_a1 <- ecdf(postdraws_wkinfo_prior$a1)\r\nemp_a1(0)\r\n\r\n\r\n[1] 0.05825\r\n\r\n7. Fig. 2: Visualization of Bayesian inference\r\n7.1. Data preparations\r\nThe following code generates the density estimates, for the a1 posterior panels\r\n\r\n\r\n# helper function for panel D\r\nget_dens <- function(ppd, probs = c(0.05, 0.95)) {\r\n  dens <- density(ppd)\r\n  mode3 <- dens$x[dens$y == max(dens$y)]\r\n  mode3.den <- dens$y[dens$x == mode3]\r\n  dens.df <- data.frame(x = dens$x, y = dens$y)\r\n  quantiles <- quantile(ppd, prob = probs)\r\n  dens.df$quant <- factor(findInterval(dens.df$x, quantiles))\r\n  dens.df$Q1 <- quantiles[1]\r\n  dens.df$Q2 <- quantiles[2]\r\n  return(dens.df)\r\n}\r\n\r\npost_dens <- get_dens(postdraws_vweak_prior$a1)\r\npost_dens2 <- get_dens(postdraws_wkinfo_prior$a1)\r\n\r\nmax_dens <- max(c(post_dens$y, post_dens2$y))\r\n\r\n\r\n\r\nThe following code generates the density estimates, for the prior panels\r\n\r\n\r\nx_length <- seq(-300, 300, length.out = 1000)\r\nx_length2 <- seq(0, 100, length.out = 1000)\r\npriors <- data.frame(\r\n  a_den = dnorm(x = x_length, mean = 0, sd = 100),\r\n  sigma_den = dexp(x = x_length2, rate = (1/21)),\r\n  x1 = x_length,\r\n  x2 = x_length2\r\n)\r\npriors2 <- data.frame(\r\n  a_den = dnorm(x = x_length, mean = 0, sd = 4),\r\n  sigma_den = dexp(x = x_length2, rate = (1/21)),\r\n  x1 = x_length,\r\n  x2 = x_length2\r\n)\r\n\r\nmax_dens_a_prior <- max(c(priors$a_den, priors2$a_den))\r\n\r\n\r\n\r\nGetting the plot ranges\r\n\r\n\r\nrange_a0 <- c(\r\n  min(min(postdraws_vweak_prior$a0), min(postdraws_vweak_prior$a0)), \r\n  max(max(postdraws_vweak_prior$a0), max(postdraws_vweak_prior$a0))\r\n)\r\nrange_a1 <- c(\r\n  min(min(postdraws_vweak_prior$a1), min(postdraws_vweak_prior$a1)), \r\n  max(max(postdraws_vweak_prior$a1), max(postdraws_vweak_prior$a1))\r\n)\r\n\r\n\r\n\r\n7.2. Fig.2, Subfigure 1\r\n\r\n\r\nf2.s1.panC <-\r\n  ggplot(data = postdraws_vweak_prior, aes(x = a0, y = a1)) +\r\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\", colour = \"white\") +\r\n  annotate(\"point\", x = 1, y = 2, color = \"black\", size = 2, shape = 15) +\r\n  annotate(\"text\", x = 1.4, y = 1.9, label = \"Truth (1, 2)\", hjust = 0, color = \"black\", size = 3) +\r\n  annotate(\"point\",\r\n    x = mean(postdraws_vweak_prior$a0), y = mean(postdraws_vweak_prior$a1),\r\n    color = \"black\", size = 2, shape = 17\r\n  ) +\r\n  annotate(\"text\",\r\n    x = -5, y = 10,\r\n    label = paste0(\r\n      \"Posterior Mean\\n(\",\r\n      round(mean(postdraws_vweak_prior$a0), 1),\r\n      \", \",\r\n      round(mean(postdraws_vweak_prior$a1), 1),\r\n      \")\"\r\n    ),\r\n    hjust = 0.5, color = \"black\", size = 3\r\n  ) +\r\n  annotate(\"segment\",\r\n    x = mean(postdraws_vweak_prior$a0), xend = -5,\r\n    y = mean(postdraws_vweak_prior$a1), yend = 8.5\r\n  ) +\r\n  annotate(\"point\", x = 0, y = 0, color = \"black\", size = 2, shape = 15) +\r\n  annotate(\"text\", x = 0.4, y = -0.1, label = \"H0 (0, 0)\", hjust = 0, color = \"black\", size = 3) +\r\n  scale_x_continuous(expand = c(0, 0), limits = range_a0) +\r\n  scale_y_continuous(expand = c(0, 0), limits = range_a1) +\r\n  scale_fill_continuous(low = \"grey95\", high = \"grey30\") +\r\n  theme(legend.position = \"none\") +\r\n  labs(\r\n    x = expression(a[0]),\r\n    y = expression(a[1]),\r\n    subtitle = quote(\"Posterior: Pr(\" * a[0] * \",\" * a[1] * \" | y, x)\")\r\n  )\r\nf2.s1.panA <-\r\n  ggplot(data = priors, aes(x = x1, y = a_den)) +\r\n  geom_area(color = \"black\", fill = \"grey30\", alpha = 0.5) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.001, 0.05)), limits = c(0, max_dens_a_prior) ) +\r\n  annotate(\"text\", x = 290, y = 0.095, label = \"N(0, 100)\", hjust = 1, color = \"black\", size = 3) +\r\n  scale_x_continuous(expand = c(0, 0), breaks = c(-300, -150, 0, 150, 300)) +\r\n  labs(\r\n    y = \"Density\",\r\n    x = expression(a[k]),\r\n    subtitle = quote(\"Prior: Pr(\" * a[k] * \")\")\r\n  )\r\nf2.s1.panB <-\r\n  ggplot(data = priors, aes(x = x2, y = sigma_den)) +\r\n  geom_area(color = \"black\", fill = \"grey30\", alpha = 0.5) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.01, 0.05))) +\r\n  scale_x_continuous(expand = c(0, 0)) +\r\n  annotate(\"text\", x = 99, y = 0.045, label = \"Exp(1/21)\", hjust = 1, color = \"black\", size = 3) +\r\n  labs(\r\n    y = \"Density\",\r\n    x = expression(sigma),\r\n    subtitle = quote(\"Prior: Pr(\" * sigma * \")\")\r\n  )\r\nf2.s1.panD <-\r\n  post_dens %>%\r\n  ggplot(aes(x = x)) +\r\n  geom_ribbon(aes(ymin = 0, ymax = y, fill = quant), alpha = 0.7) +\r\n  geom_line(aes(y = y), color = \"black\") +\r\n  labs(\r\n    x = expression(a[1]),\r\n    y = expression(\"Density\"),\r\n    subtitle = quote(\"Posterior: Pr(\" * a[1] * \" | y, x)\")\r\n  ) +\r\n  scale_x_continuous(expand = c(0, 0)) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.0, 0.05))) +\r\n  scale_fill_manual(values = c(\"grey50\", \"grey70\", \"grey50\")) +\r\n  theme(legend.position = \"None\") +\r\n  annotate(\"segment\", x = 7, xend = 12, y = 0.02, yend = 0.05) +\r\n  annotate(\"text\", x = 11.5, y = 0.052, label = \"90% Credible\\ninterval\", color = \"black\", size = 3, hjust = 0) +\r\n  coord_flip(xlim = range_a1, ylim = c(0, max_dens))\r\n\r\nlayout <- \"\r\nACCD\r\nBCCD\r\n\"\r\n\r\nfig2.s1 <- f2.s1.panA + f2.s1.panB + f2.s1.panC + f2.s1.panD +\r\n  plot_layout(design = layout) +\r\n  plot_annotation(tag_levels = \"A\")\r\n\r\n\r\n\r\n7.3. Fig.2, Subfigure 2\r\n\r\n\r\nf2.s2.panC <-\r\n  ggplot(data = postdraws_wkinfo_prior, aes(x = a0, y = a1)) +\r\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\", colour = \"white\") +\r\n  annotate(\"point\", x = 1, y = 2, color = \"black\", size = 2, shape = 15) +\r\n  annotate(\"text\", x = 1.4, y = 1.9, label = \"Truth (1, 2)\", hjust = 0, color = \"black\", size = 3) +\r\n  annotate(\"point\",\r\n    x = mean(postdraws_wkinfo_prior$a0), y = mean(postdraws_wkinfo_prior$a1),\r\n    color = \"black\", size = 2, shape = 17\r\n  ) +\r\n  annotate(\"text\",\r\n    x = -5, y = 10,\r\n    label = paste0(\r\n      \"Posterior Mean\\n(\",\r\n      round(mean(postdraws_wkinfo_prior$a0), 1),\r\n      \", \",\r\n      round(mean(postdraws_wkinfo_prior$a1), 1),\r\n      \")\"\r\n    ),\r\n    hjust = 0.5, color = \"black\", size = 3\r\n  ) +\r\n  annotate(\"segment\",\r\n    x = mean(postdraws_wkinfo_prior$a0), xend = -5,\r\n    y = mean(postdraws_wkinfo_prior$a1), yend = 8.5\r\n  ) +\r\n  annotate(\"point\", x = 0, y = 0, color = \"black\", size = 2, shape = 15) +\r\n  annotate(\"text\", x = 0.4, y = -0.1, label = \"H0 (0, 0)\", hjust = 0, color = \"black\", size = 3) +\r\n  scale_x_continuous(expand = c(0, 0), limits = range_a0) +\r\n  scale_y_continuous(expand = c(0, 0), limits = range_a1) +\r\n  scale_fill_continuous(low = \"grey95\", high = \"grey30\") +\r\n  theme(legend.position = \"none\") +\r\n  labs(\r\n    x = expression(a[0]),\r\n    y = expression(a[1]),\r\n    subtitle = quote(\"Posterior: Pr(\" * a[0] * \",\" * a[1] * \" | y, x)\")\r\n  )\r\nf2.s2.panA <-\r\n  ggplot(data = priors2, aes(x = x1, y = a_den)) +\r\n  geom_area(color = \"black\", fill = \"grey30\", alpha = 0.5) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.001, 0.05)), limits = c(0, max_dens_a_prior)) +\r\n  annotate(\"text\", x = 290, y = 0.095, label = \"N(0, 4)\", hjust = 1, color = \"black\", size = 3) +\r\n  scale_x_continuous(expand = c(0, 0), breaks = c(-300, -150, 0, 150, 300)) +\r\n  labs(\r\n    y = \"Density\",\r\n    x = expression(a[k]),\r\n    subtitle = quote(\"Prior: Pr(\" * a[1] * \")\")\r\n  )\r\nf2.s2.panB <-\r\n  ggplot(data = priors2, aes(x = x2, y = sigma_den)) +\r\n  geom_area(color = \"black\", fill = \"grey30\", alpha = 0.5) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.01, 0.05))) +\r\n  scale_x_continuous(expand = c(0, 0)) +\r\n  annotate(\"text\", x = 99, y = 0.045, label = \"Exp(1/21)\", hjust = 1, color = \"black\", size = 3) +\r\n  labs(\r\n    y = \"Density\",\r\n    x = expression(sigma),\r\n    subtitle = quote(\"Prior: Pr(\" * sigma * \")\")\r\n  )\r\nf2.s2.panD <-\r\n  post_dens2 %>%\r\n  ggplot(aes(x = x)) +\r\n  geom_ribbon(aes(ymin = 0, ymax = y, fill = quant), alpha = 0.7) +\r\n  geom_line(aes(y = y), color = \"black\") +\r\n  labs(\r\n    x = expression(a[1]),\r\n    y = expression(\"Density\"),\r\n    subtitle = quote(\"Posterior: Pr(\" * a[1] * \" | y, x)\")\r\n  ) +\r\n  scale_x_continuous(expand = c(0, 0), limits = range_a1) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.0, 0.05))) +\r\n  scale_fill_manual(values = c(\"grey50\", \"grey70\", \"grey50\")) +\r\n  theme(legend.position = \"None\") +\r\n  annotate(\"segment\", x = 7, xend = 12, y = 0.02, yend = 0.05) +\r\n  annotate(\"text\", x = 11.5, y = 0.052, label = \"90% Credible\\ninterval\", color = \"black\", size = 3, hjust = 0) +\r\n  coord_flip(xlim = range_a1, ylim = c(0, max_dens))\r\n\r\nlayout <- \"\r\nACCD\r\nBCCD\r\n\"\r\n\r\nfig2.s2 <- f2.s2.panA + f2.s2.panB + f2.s2.panC + f2.s2.panD +\r\n  plot_layout(design = layout) +\r\n  plot_annotation(tag_levels = \"A\")\r\n\r\n\r\n\r\n\r\n\r\nfig2.s1\r\n\r\n\r\n\r\n\r\n\r\n\r\nfig2.s2\r\n\r\n\r\n\r\n\r\nSaving subfigures\r\n\r\n\r\nsave_fig(fig=fig2.s1, figname = \"fig2-1\", w = 6.2, h = 3.3)\r\nsave_fig(fig=fig2.s2, figname = \"fig2-2\", w = 6.2, h = 3.3)\r\n\r\n\r\n\r\n8. Fig. 3: Priors vs Likelihood\r\n8.1. Generate a larger sample\r\n\r\n\r\nset.seed(999)\r\nnew_obs <- 150\r\nx_fix2 <- rnorm(n = new_obs, 0, 1)\r\nsample2 <- tibble(u = rnorm(new_obs, 0, 20), x = x_fix2, y = 1 + 2 * x_fix2 + u)\r\nsample2 <- bind_rows(sample1, sample2)\r\nn_obs2 <- new_obs + n_obs\r\n\r\ninput_data2 <- list(\r\n  y = sample2$y,\r\n  x = sample2$x,\r\n  N = nrow(sample2)\r\n)\r\n\r\n\r\n\r\n8.2. Fitting the weakly informative model to larger sample\r\n\r\n\r\nfit_wkinfo_priors_large <- model_wkinfo_priors$sample(\r\n  data = input_data2,\r\n  iter_sampling = 1000,\r\n  iter_warmup = 1000,\r\n  chains = 4,\r\n  parallel_chains = 4,\r\n  seed = 1234,\r\n  refresh = 1000\r\n)\r\n\r\n\r\nRunning MCMC with 4 parallel chains...\r\n\r\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 1 finished in 0.2 seconds.\r\nChain 2 finished in 0.1 seconds.\r\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 3 finished in 0.1 seconds.\r\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 4 finished in 0.1 seconds.\r\n\r\nAll 4 chains finished successfully.\r\nMean chain execution time: 0.1 seconds.\r\nTotal execution time: 0.3 seconds.\r\n\r\n\r\n\r\npostdraws_wkinfo_prior_large <- fit_wkinfo_priors_large$draws(format = \"draws_df\")\r\nfit_wkinfo_priors_large$summary(variables = c(\"sigma\", \"a0\", \"a1\")) |> \r\n  mutate(across(where(is.numeric), round, 3))  |> \r\n  kable()\r\n\r\n\r\nvariable\r\nmean\r\nmedian\r\nsd\r\nmad\r\nq5\r\nq95\r\nrhat\r\ness_bulk\r\ness_tail\r\nsigma\r\n19.649\r\n19.617\r\n0.988\r\n1.000\r\n18.088\r\n21.313\r\n0.999\r\n4670.806\r\n2760.187\r\na0\r\n-0.136\r\n-0.149\r\n1.383\r\n1.358\r\n-2.424\r\n2.125\r\n1.001\r\n3777.942\r\n2872.741\r\na1\r\n2.467\r\n2.467\r\n1.324\r\n1.337\r\n0.317\r\n4.634\r\n1.000\r\n3953.647\r\n3027.360\r\n\r\n8.3. Collecting Prior, likelihood, posterior for small, current sample\r\n\r\n\r\nloglik_a1 <- function(a1, x, y, a0, u_sd) {\r\n  resids <- y - (a0 + a1 * x)\r\n  log_lik_vec <- dnorm(x = resids, mean = 0, sd = u_sd, log = TRUE)\r\n  log_lik <- sum(log_lik_vec)\r\n  return(log_lik)\r\n}\r\n\r\n\r\n\r\n\r\n\r\nols_model_small <- lm(y ~ x, data = sample1)\r\na0 <- ols_model_small$coefficients[[\"(Intercept)\"]]\r\nusd <- sd(ols_model_small$residuals)\r\na1_hat <- ols_model_small$coefficients[[\"x\"]]\r\n\r\nsmall_sample_data <- tibble(a1 = seq(-20, +30, length.out = 1000))\r\nsmall_sample_data$loglik <- map_dbl(\r\n  small_sample_data$a1,\r\n  loglik_a1,\r\n  x = sample1$x,\r\n  y = sample1$y,\r\n  a0 = a0,\r\n  u_sd = usd\r\n)\r\nsmall_sample_data <-\r\n  small_sample_data %>%\r\n  mutate(\r\n    # norm_lik = -2 * loglik, #exp(loglik) / sum(exp(loglik)),\r\n    norm_lik = (loglik),\r\n    prior = dnorm(a1, 0, 4),\r\n    post = approxfun(density(postdraws_wkinfo_prior$a1))(a1)\r\n  ) \r\nsmall_sample_data$post[is.na(small_sample_data$post)] <- 0\r\n\r\n\r\n\r\n8.4. Collecting Prior, likelihood, posterior for larger sample\r\n\r\n\r\nols_model_large <- lm(y ~ x, data = sample2)\r\na0 <- ols_model_large$coefficients[[\"(Intercept)\"]]\r\nusd <- sd(ols_model_large$residuals)\r\na1_hat_big <- ols_model_large$coefficients[[\"x\"]]\r\n\r\nlarge_sample_data <- tibble(a1 = seq(-20, +30, length.out = 1000))\r\nlarge_sample_data$loglik <- map_dbl(\r\n  large_sample_data$a1,\r\n  loglik_a1,\r\n  x = sample2$x,\r\n  y = sample2$y,\r\n  a0 = a0,\r\n  u_sd = usd\r\n)\r\nlarge_sample_data <-\r\n  large_sample_data %>%\r\n  mutate(\r\n    # norm_lik = -2 * loglik, #exp(loglik) / sum(exp(loglik)),\r\n    norm_lik = (loglik),\r\n    prior = dnorm(a1, 0, 4),\r\n    post = approxfun(density(postdraws_wkinfo_prior_large$a1))(a1)\r\n  ) \r\nlarge_sample_data$post[is.na(large_sample_data$post)] <- 0\r\n\r\n\r\n\r\n8.5. Fig. 3, Panel A and Panel B\r\n\r\n\r\nSCALER_1 <- -2 * 0.0001\r\nf3.panA <- \r\n  small_sample_data %>%\r\n  mutate(norm_lik = (-1600 - norm_lik) * SCALER_1) %>%\r\n  pivot_longer(c(norm_lik, prior, post), names_to = \"part\") %>%\r\n  ggplot(aes(x = a1, y = value, group = part, color = part, fill = part)) +\r\n  geom_ribbon(aes(ymin = 0, ymax = value), alpha = 0.5) +\r\n  geom_line() + \r\n  geom_vline(xintercept = a1_hat, color = \"grey80\", linetype = \"dashed\") +\r\n  labs(\r\n    y = \"Density\",\r\n    x = expression(a[1]),\r\n    subtitle = \"Small sample (n = 50)\"\r\n  ) +\r\n  annotate(\"text\",\r\n    x = c(-10, -4, 17),\r\n    y = c(0.1, 0.2, 0.20),\r\n    label = c(\"Prior\", \"Posterior\", \"Log likelihood\"),\r\n    hjust = 0.5,\r\n    size = 3\r\n  ) +\r\n  annotate(\"segment\",\r\n    x = c(-10, -4, 17),\r\n    y = c(0.090, 0.189, 0.21),\r\n    xend = c(-3.8, 2.5, 10),\r\n    yend = c(0.050, 0.13, 0.27),\r\n    color = c(\"grey50\", \"grey30\", \"grey80\")\r\n  ) +\r\n  annotate(\"text\",\r\n    x = a1_hat + 0.1,\r\n    y = 0.31, label = paste(\"Max likelihood at\", round(a1_hat, 1)),\r\n    hjust = 0, color = \"grey50\", size = 3\r\n  )\r\n  \r\n\r\nf3.panB <- \r\n  large_sample_data %>%\r\n  mutate(norm_lik = (-1600 - norm_lik) * SCALER_1) %>%\r\n  pivot_longer(c(norm_lik, prior, post), names_to = \"part\") %>%\r\n  ggplot(aes(x = a1, y = value, group = part, color = part, fill = part)) +\r\n  geom_ribbon(aes(ymin = 0, ymax = value), alpha = 0.5) +\r\n  geom_line() +\r\n  geom_vline(xintercept = a1_hat_big, color = \"grey80\", linetype = \"dashed\") +\r\n  labs(\r\n    y = \"Density\",\r\n    x = expression(a[1]),\r\n    subtitle = \"Large Sample (n = 200)\"\r\n  ) +\r\n  annotate(\"text\",\r\n    x = c(-10, -6, 17),\r\n    y = c(0.1, 0.2, 0.20),\r\n    label = c(\"Prior\", \"Posterior\", \"Log likelihood\"),\r\n    hjust = 0.5,\r\n    size = 3\r\n  ) +\r\n  annotate(\"segment\",\r\n    x = c(-10, -6, 17),\r\n    y = c(0.090, 0.189, 0.19),\r\n    xend = c(-3.8, 1.5, 10),\r\n    yend = c(0.050, 0.13, 0.13),\r\n    color = c(\"grey50\", \"grey50\", \"grey80\")\r\n  ) +\r\n  annotate(\"text\",\r\n    x = a1_hat_big + .1,\r\n    y = 0.31, label = paste(\"Max likelihood at\", round(a1_hat_big, 1)),\r\n    hjust = 0, color = \"grey50\", size = 3\r\n  ) \r\n\r\nfig3 <- \r\n  f3.panA + f3.panB +\r\n  plot_annotation(tag_levels = \"A\") &\r\n  annotate(\"segment\", x = 2, y = 0, xend = 2, yend = 0.02) &\r\n  theme(legend.position = \"none\", aspect.ratio=1) & \r\n  scale_color_manual(values = c(\"grey80\", \"grey30\", \"grey50\")) &\r\n  scale_fill_manual(values = c(\"grey80\", \"grey30\", \"grey50\")) &\r\n  scale_y_continuous(\r\n    expand = expansion(mult = c(0, 0.05)),\r\n    limits = c(0, 0.32),\r\n    sec.axis = sec_axis(~ -1600 - . / SCALER_1, name = \"Log likelihood\")\r\n  ) &\r\n  scale_x_continuous(\r\n    breaks = c(-20, -10, 0, 2, 10, 20, 30),\r\n    expand = expansion(mult = c(0, 0))\r\n  )\r\nfig3\r\n\r\n\r\n\r\n\r\nSaving figure\r\n\r\n\r\nsave_fig(fig=fig3, figname = \"fig3\", w = 6.2, h = 3.1)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-10-03T15:31:18+02:00"
    },
    {
      "path": "index.html",
      "title": "Bayesian Inference for Accounting Research",
      "description": "Welcome to the companion website for the paper *What Can Bayesian Inference Do for Accounting Research?* I hope this site and the accompanying [GitHub repo](https://github.com/hschuett/BayesForAccountingResearch) will help those interested in Bayesian methods to find a starting point. Hopefully you find it useful.\n",
      "author": [
        {
          "name": "Harm Schuett",
          "url": "https://hschuett.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\nThe site is organized as follows. The first part provides the annotated code to produce the output and figures used in the paper (the GitHub repo also contains the code to compute the data used). Code can be copied easily to the clipboard and be reused. The second part is still under development and contains additional material useful to those interested in Bayesian analysis.\r\nPaper code\r\nSection 2: Simulation contains all the code to replicate the figures and results from Section 2, which compares Classical hypothesis testing to Bayesian inference.\r\nAdditional material\r\n\r\n\r\n\r\n",
      "last_modified": "2021-10-03T15:31:18+02:00"
    }
  ],
  "collections": []
}
