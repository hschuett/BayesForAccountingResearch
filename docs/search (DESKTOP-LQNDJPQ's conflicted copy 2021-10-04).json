{
  "articles": [
    {
      "path": "classic-vs-bayes.html",
      "title": "Simulation: Classical vs. Bayesian Regression",
      "description": "Code for the Section 2: Bayesian inference compared to classical inference\n",
      "author": [
        {
          "name": "Harm Schuett",
          "url": "https://hschuett.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\n1. Introduction and preliminaries\r\n2. Creating the simulated data\r\n3. OLS regressions\r\n3.1. OLS fits of all 50 samples\r\n3.2. OlS estimates of the first sample\r\n\r\n4. Fig. 1: Classical hypothesis tests\r\n4.1. Fig.1, Panel A\r\n4.2. Fig.1, Panel B\r\n\r\n5. Additional t-test figure\r\n6. Fitting the Bayesian models\r\n6.1 Bayesian model with diffuse/very weak priors\r\n6.2. Bayesian model with weakly informative priors\r\n\r\n7. Fig. 2: Visualization of Bayesian inference\r\n7.1. Data preparations\r\n7.2. Fig.2, Subfigure 1\r\n7.3. Fig.2, Subfigure 2\r\n\r\n8. Fig. 3: Priors vs Likelihood\r\n8.1. Generate a larger sample\r\n8.2. Fitting the weakly informative model to larger sample\r\n8.3. Collecting Prior, likelihood, posterior for small, current sample\r\n8.4. Collecting Prior, likelihood, posterior for larger sample\r\n8.5. Fig. 3, Panel A and Panel B\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(broom)\r\nlibrary(patchwork)\r\nlibrary(cmdstanr)\r\nsource(\"../R/00-utils.R\")\r\nkable <- knitr::kable\r\ntheme_set(theme_prodgray())\r\n\r\n\r\n\r\n1. Introduction and preliminaries\r\nThis markdown file contains all the code necessary to replicate the figures, models and results used in Section 2: Bayesian inference compared to classical inference of the Paper What Can Bayesian Inference Do for Accounting Research?. All the code can also be found in the repo. It contains 00-utils.R which contains a few helper functions for graphs and tables.\r\nNote: I started using the newer cmdstanr package instead of the older rstan package because it likely is the future of the R based Stan ecosystem. I also really like its api, which is very close to the api of the pystan package. An additional advantage (I hope) is thus that most model fitting code should be more or less directly transferable to pystan for those that want to work in python.\r\nInstalling cmdstanr used to be tricky at times because one needs a working c++ toolchain. But it is much smoother now. Please see the cmdstanr doc for installation instructions\r\n2. Creating the simulated data\r\nFirst we create the 50 samples of 50 observations each\r\n\r\n\r\nset.seed(888)\r\nn_samples <- 50\r\nn_obs <- 50\r\nx_fix <- rnorm(n = n_obs, 0, 1)\r\ngen_data <- function(n) tibble(u = rnorm(n, 0, 20), x = x_fix, y = 1 + 2 * x_fix + u)\r\n\r\n# gen samples\r\nsamples <- tibble(id = 1:n_samples)\r\nsamples$data <- map(rep.int(n_obs, n_samples), gen_data)\r\n\r\n\r\n\r\nNext we create the relevant figures\r\n3. OLS regressions\r\n3.1. OLS fits of all 50 samples\r\n\r\n\r\n# gen ols estimates\r\nsamples$ols_fit <- map(samples$data, function(dat) tidy(lm(y ~ x, data = dat)))\r\nols_estimates <- samples %>%\r\n  unnest(ols_fit) %>%\r\n  mutate(param = if_else(term == \"x\", \"a1\", \"a0\")) %>%\r\n  select(id, param, estimate) %>%\r\n  spread(key = param, value = estimate) %>%\r\n  mutate(\r\n    plabs = paste0(\"(\", round(a0, 1), \",\", round(a1, 1), \")\"),\r\n    first_sample = as.factor(c(1, rep.int(0, times = n_samples - 1)))\r\n  )\r\n\r\n\r\n\r\n3.2. OlS estimates of the first sample\r\n\r\n\r\nkable(samples$ols_fit[[1]])\r\n\r\n\r\nterm\r\nestimate\r\nstd.error\r\nstatistic\r\np.value\r\n(Intercept)\r\n0.7602868\r\n2.873058\r\n0.2646264\r\n0.7924303\r\nx\r\n4.3777130\r\n2.653451\r\n1.6498189\r\n0.1055098\r\n\r\n4. Fig. 1: Classical hypothesis tests\r\nWe need the following variables for scaling Fig. 1\r\n\r\n\r\nrange_x <- range(ols_estimates$a0)\r\nrange_y <- range(ols_estimates$a1)\r\nnudge_x <- 0.1\r\nsample_a0 <- ols_estimates$a0[1]\r\nsample_a1 <- ols_estimates$a1[1]\r\nr <- sd(ols_estimates$a1)\r\n\r\n\r\n\r\n4.1. Fig.1, Panel A\r\n\r\n\r\nf1.panA <-\r\n  ols_estimates %>%\r\n  ggplot(aes(x = a0, y = a1, shape = first_sample, color = first_sample)) +\r\n  geom_point(size = 2) +\r\n  xlim(range_x) +\r\n  ylim(range_y) +\r\n  scale_color_manual(values = c(\"gray80\", \"black\")) +\r\n  labs(\r\n    x = expression(\" Intercept estimate \" ~ hat(a)[0]),\r\n    y = expression(\" Slope estimate \" ~ hat(a)[1]),\r\n    subtitle = \"Sampling variation\"\r\n  ) +\r\n  geom_segment(aes(color = first_sample), xend = 1, yend = 2, linetype = 2, alpha = 0.5) +\r\n  annotate(\"point\", x = 1, y = 2, color = \"black\", size = 2, shape = 15) +\r\n  annotate(\"text\", x = 1.4, y = 1.9, label = \"Truth (1, 2)\", hjust = 0, color = \"black\", size = 3) +\r\n  annotate(\"text\",\r\n    x = sample_a0 + 0.4,\r\n    y = sample_a1 + -0.1,\r\n    label = paste0(\"Sample (\", round(sample_a0, 1), \", \", round(sample_a1, 1), \")\"),\r\n    hjust = 0, color = \"black\", size = 3\r\n  ) +\r\n  annotate(\"point\", x = 0, y = 0, color = \"black\", size = 2, shape = 15) +\r\n  annotate(\"text\", x = 0.4, y = -0.1, label = \"H0 (0, 0)\", hjust = 0, color = \"black\", size = 3) +\r\n  theme(legend.position = \"none\", aspect.ratio=1)\r\n\r\n\r\n\r\n4.2. Fig.1, Panel B\r\n\r\n\r\nf1.panB <- \r\n  ols_estimates %>%\r\n  ggplot(aes(x = a0, y = a1, shape = first_sample, color = first_sample)) +\r\n  geom_point(size = 2) +\r\n  xlim(range_x) +\r\n  ylim(range_y) +\r\n  scale_color_manual(values = c(\"gray80\", \"black\")) +\r\n  labs(\r\n    x = expression(\" Intercept estimate \" ~ hat(a)[0]),\r\n    y = expression(\" Slope estimate \" ~ hat(a)[1]),\r\n    subtitle = \"Distance from hypothesis\"\r\n  ) +\r\n  theme(legend.position = \"none\") +\r\n  geom_segment(aes(color = first_sample), xend = 0, yend = 0, linetype = 2, alpha = 0.5) +\r\n  annotate(\"point\", x = 0, y = 0, color = \"black\", size = 2, shape = 15) +\r\n  annotate(\"text\", x = 0.4, y = -0.1, label = \"H0 (0, 0)\", hjust = 0, color = \"black\", size = 3) +\r\n  annotate(\"text\",\r\n    x = sample_a0 + 0.4,\r\n    y = sample_a1 + -0.1,\r\n    label = paste0(\"Sample (\", round(sample_a0, 1), \", \", round(sample_a1, 1), \")\"),\r\n    hjust = 0, color = \"black\", size = 3\r\n  ) +\r\n  geom_hline(yintercept = c(r, -r), color = \"black\") +\r\n  geom_segment(\r\n    x = -4, xend = -4, y = -r, yend = r, color = \"black\",\r\n    arrow = arrow(length = unit(0.07, \"inches\"), ends = \"both\", type = \"closed\")\r\n  ) +\r\n  annotate(\"text\",\r\n    x = -3.4, y = -2.2,\r\n    size = 3, hjust = 0, vjust = 0, color = \"black\",\r\n    label = \"One standard deviation\"\r\n  ) +\r\n  theme(legend.position = \"none\", aspect.ratio=1)\r\n\r\n\r\n\r\nSaving figure\r\n\r\n\r\nfig1 <- f1.panA + f1.panB + plot_annotation(tag_levels = \"A\")\r\nsave_fig(fig1, figname = \"fig1\", w = 6.2, h = 3.3)\r\n\r\n\r\n\r\n\r\n\r\nfig1\r\n\r\n\r\n\r\n\r\n5. Additional t-test figure\r\nThis one is not in the paper. I used it for an internal Ph.D. seminar\r\n\r\n\r\nt_dist <- data.frame(ps = rt(n=10000, df=n_obs-2))\r\ntest_stat <- 1.65\r\nquant_test <- round(1 - pt(test_stat, df=n_obs-2), 2)\r\n\r\nfZ <- \r\n  ggplot(data=t_dist, aes(x=ps)) +\r\n  geom_histogram(bins=50, color=\"white\", fill=\"gray30\") +\r\n  geom_segment(color=\"red\",\r\n               x=test_stat, y=0,\r\n               xend=test_stat, yend=200) +\r\n  geom_segment(color=\"red\",\r\n               x=-test_stat, y=0,\r\n               xend=-test_stat, yend=200) +\r\n  # geom_area(color=high_red, fill=high_red, alpha=0.5) +\r\n  annotate(\"text\", x=test_stat, y=200,\r\n           size=3, hjust=0, vjust=0,\r\n           label=paste0(\"P(t >= \", test_stat, \" | H0) = \", quant_test)) +\r\n  annotate(\"text\", x=-test_stat, y=200,\r\n           size=3, hjust=1, vjust=0,\r\n           label=paste0(\"P(t <= -\", test_stat, \" | H0) = \", quant_test)) +\r\n  scale_y_continuous(expand=c(0,0)) +\r\n  labs(x = expression((hat(a)-H0)/SE),\r\n       y = NULL,\r\n       subtitle = \"Occurrence in repeated sampling\") +\r\n  theme(legend.position = \"none\")\r\nfZ\r\n\r\n\r\n\r\n\r\n6. Fitting the Bayesian models\r\nTo fit a Bayesian model I use Stan, or, more precisely, its R bindings in cmdstanr. To fit a Bayesian model, we need to:\r\nWrite the corresponding model using the Stan language\r\nCompile the code into a Stan model executable (an .exe file)\r\nMake a list of data to feed into the .exe file\r\nLet the model run and generate MCMC chains (or whatever algorithm is specified)\r\n6.1 Bayesian model with diffuse/very weak priors\r\n\r\n\r\nsample1 <- samples$data[[1]]\r\n\r\n\r\n\r\nThe following descriptive are useful when assessing what priors to use:\r\n\r\n\r\nsample1_descs <- c(\r\n  u_y = mean(sample1$y),\r\n  u_x = mean(sample1$x),\r\n  s_y = sd(sample1$y),\r\n  s_x = sd(sample1$x)\r\n) \r\n\r\nround(sample1_descs, 3)\r\n\r\n\r\n   u_y    u_x    s_y    s_x \r\n 0.499 -0.060 20.638  1.092 \r\n\r\nThe model itself is coded in the Stan language. There are many excellent tutorials on Stan available online. So I won’t waste space explaining it here. For various reasons (e.g., debugging) it is customary to put the model code in a separate .stan file. All the model files can be found in the /Stan/ folder of the repo.\r\n\r\n\r\ncat(read_lines(\"../Stan/sim-vweak-priors.stan\"), sep = \"\\n\")\r\n\r\n\r\ndata{\r\n  int<lower=1> N;\r\n  real y[N];\r\n  real x[N];\r\n}\r\nparameters{\r\n  real a0;\r\n  real a1;\r\n  real<lower=0> sigma;\r\n}\r\nmodel{\r\n  vector[N] mu;\r\n  sigma ~ exponential( 1.0 / 21.0 );\r\n  a0 ~ normal( 0 , 100 );\r\n  a1 ~ normal( 0 , 100 );\r\n  for ( i in 1:N ) {\r\n    mu[i] = a0 + a1 * x[i];\r\n  }\r\n  y ~ normal( mu , sigma );\r\n}\r\n\r\nThe model code in Stan language\r\nNext, we compile the model to an .exe file\r\n\r\n\r\nmodel_vweak_priors <- cmdstan_model(\"../Stan/sim-vweak-priors.stan\")\r\n\r\n\r\n\r\nNow, we prepare the list of data to feed into the model.\r\n\r\n\r\ninput_data <- list(\r\n  y = sample1$y,\r\n  x = sample1$x,\r\n  N = nrow(sample1)\r\n)\r\n\r\n\r\n\r\nNote how the list matches the names of the variables declared in the model’s data block\r\nWe run the model\r\n\r\n\r\nfit_vweak_priors <- model_vweak_priors$sample(\r\n  data = input_data,\r\n  iter_sampling = 1000,\r\n  iter_warmup = 1000,\r\n  chains = 4,\r\n  parallel_chains = 4,\r\n  seed = 1234,\r\n  refresh = 1000\r\n)\r\n\r\n\r\nRunning MCMC with 4 parallel chains...\r\n\r\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 1 finished in 0.1 seconds.\r\nChain 2 finished in 0.1 seconds.\r\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 3 finished in 0.1 seconds.\r\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 4 finished in 0.1 seconds.\r\n\r\nAll 4 chains finished successfully.\r\nMean chain execution time: 0.1 seconds.\r\nTotal execution time: 0.3 seconds.\r\n\r\nHere is the summary of the resulting posterior distribution of the model parameters\r\n\r\n\r\nfit_vweak_priors$summary(variables = c(\"sigma\", \"a0\", \"a1\")) |> \r\n  mutate(across(where(is.numeric), round, 3)) |> \r\n  kable()\r\n\r\n\r\nvariable\r\nmean\r\nmedian\r\nsd\r\nmad\r\nq5\r\nq95\r\nrhat\r\ness_bulk\r\ness_tail\r\nsigma\r\n20.556\r\n20.380\r\n2.125\r\n2.074\r\n17.446\r\n24.220\r\n1.003\r\n3094.064\r\n2515.482\r\na0\r\n0.756\r\n0.778\r\n2.921\r\n2.932\r\n-3.884\r\n5.581\r\n1.000\r\n3366.030\r\n2861.077\r\na1\r\n4.241\r\n4.241\r\n2.665\r\n2.681\r\n-0.186\r\n8.541\r\n1.000\r\n3496.024\r\n2752.886\r\n\r\nPosterior descriptives\r\nThis is one form of getting the mcmc chain output. We need the draws to draw inferences about the posterior.\r\n\r\n\r\npostdraws_vweak_prior <- fit_vweak_priors$draws(format = \"draws_df\")\r\nhead(postdraws_vweak_prior)\r\n\r\n\r\n# A draws_df: 6 iterations, 1 chains, and 4 variables\r\n  lp__     a0   a1 sigma\r\n1 -173  2.697  4.0    21\r\n2 -174 -1.066  6.8    18\r\n3 -173  0.071  5.6    19\r\n4 -174  2.678  6.5    23\r\n5 -174 -1.515  2.1    18\r\n6 -176 -1.275 -2.1    23\r\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\r\n\r\nThe posterior 90% centered credible interval:\r\n\r\n\r\nquantile(postdraws_vweak_prior$a1, probs = c(0.05, 0.95))\r\n\r\n\r\n        5%        95% \r\n-0.1861673  8.5411815 \r\n\r\nThe posterior probability that a1 < 0:\r\n\r\n\r\nemp_a1 <- ecdf(postdraws_vweak_prior$a1)\r\nemp_a1(0)\r\n\r\n\r\n[1] 0.05825\r\n\r\n6.2. Bayesian model with weakly informative priors\r\nThis is basically the same model, just with different hard-coded priors.\r\n\r\n\r\ncat(read_lines(\"../Stan/sim-wkinfo-priors.stan\"), sep = \"\\n\")\r\n\r\n\r\ndata{\r\n  int<lower=1> N;\r\n  real y[N];\r\n  real x[N];\r\n}\r\nparameters{\r\n  real a0;\r\n  real a1;\r\n  real<lower=0> sigma;\r\n}\r\nmodel{\r\n  vector[N] mu;\r\n  sigma ~ exponential( 1.0 / 21.0 );\r\n  a0 ~ normal( 0 , 100 );\r\n  a1 ~ normal( 0 , 4 );\r\n  for ( i in 1:N ) {\r\n    mu[i] = a0 + a1 * x[i];\r\n  }\r\n  y ~ normal( mu , sigma );\r\n}\r\n\r\n\r\n\r\nmodel_wkinfo_priors <- cmdstan_model(\"../Stan/sim-wkinfo-priors.stan\")\r\n\r\n\r\n\r\nBecause we use the same data (The list input_data), we can fit the new model now:\r\n\r\n\r\nfit_wkinfo_priors <- model_wkinfo_priors$sample(\r\n  data = input_data,\r\n  iter_sampling = 1000,\r\n  iter_warmup = 1000,\r\n  chains = 4,\r\n  parallel_chains = 4,\r\n  seed = 1234,\r\n  refresh = 1000\r\n)\r\n\r\n\r\nRunning MCMC with 4 parallel chains...\r\n\r\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 1 finished in 0.1 seconds.\r\nChain 2 finished in 0.1 seconds.\r\nChain 3 finished in 0.1 seconds.\r\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 4 finished in 0.1 seconds.\r\n\r\nAll 4 chains finished successfully.\r\nMean chain execution time: 0.1 seconds.\r\nTotal execution time: 0.3 seconds.\r\n\r\nHere is the summary of the resulting posterior distribution of the model parameters\r\n\r\n\r\nfit_wkinfo_priors$summary(variables = c(\"sigma\", \"a0\", \"a1\")) |> \r\n  mutate(across(where(is.numeric), round, 3)) |> \r\n  kable()\r\n\r\n\r\nvariable\r\nmean\r\nmedian\r\nsd\r\nmad\r\nq5\r\nq95\r\nrhat\r\ness_bulk\r\ness_tail\r\nsigma\r\n20.567\r\n20.358\r\n2.161\r\n2.072\r\n17.333\r\n24.506\r\n1.001\r\n4276.167\r\n3167.417\r\na0\r\n0.672\r\n0.690\r\n2.878\r\n2.897\r\n-4.040\r\n5.364\r\n1.000\r\n4199.116\r\n2467.601\r\na1\r\n2.978\r\n3.013\r\n2.213\r\n2.186\r\n-0.793\r\n6.560\r\n1.001\r\n4074.289\r\n3219.891\r\n\r\nPosterior descriptives\r\nThe posterior 90% centered credible interval:\r\n\r\n\r\npostdraws_wkinfo_prior <- fit_vweak_priors$draws(format = \"draws_df\")\r\nquantile(postdraws_wkinfo_prior$a1, probs = c(0.05, 0.95))\r\n\r\n\r\n        5%        95% \r\n-0.1861673  8.5411815 \r\n\r\nThe posterior probability that a1 < 0:\r\n\r\n\r\nemp_a1 <- ecdf(postdraws_wkinfo_prior$a1)\r\nemp_a1(0)\r\n\r\n\r\n[1] 0.05825\r\n\r\n7. Fig. 2: Visualization of Bayesian inference\r\n7.1. Data preparations\r\nThe following code generates the density estimates, for the a1 posterior panels\r\n\r\n\r\n# helper function for panel D\r\nget_dens <- function(ppd, probs = c(0.05, 0.95)) {\r\n  dens <- density(ppd)\r\n  mode3 <- dens$x[dens$y == max(dens$y)]\r\n  mode3.den <- dens$y[dens$x == mode3]\r\n  dens.df <- data.frame(x = dens$x, y = dens$y)\r\n  quantiles <- quantile(ppd, prob = probs)\r\n  dens.df$quant <- factor(findInterval(dens.df$x, quantiles))\r\n  dens.df$Q1 <- quantiles[1]\r\n  dens.df$Q2 <- quantiles[2]\r\n  return(dens.df)\r\n}\r\n\r\npost_dens <- get_dens(postdraws_vweak_prior$a1)\r\npost_dens2 <- get_dens(postdraws_wkinfo_prior$a1)\r\n\r\nmax_dens <- max(c(post_dens$y, post_dens2$y))\r\n\r\n\r\n\r\nThe following code generates the density estimates, for the prior panels\r\n\r\n\r\nx_length <- seq(-300, 300, length.out = 1000)\r\nx_length2 <- seq(0, 100, length.out = 1000)\r\npriors <- data.frame(\r\n  a_den = dnorm(x = x_length, mean = 0, sd = 100),\r\n  sigma_den = dexp(x = x_length2, rate = (1/21)),\r\n  x1 = x_length,\r\n  x2 = x_length2\r\n)\r\npriors2 <- data.frame(\r\n  a_den = dnorm(x = x_length, mean = 0, sd = 4),\r\n  sigma_den = dexp(x = x_length2, rate = (1/21)),\r\n  x1 = x_length,\r\n  x2 = x_length2\r\n)\r\n\r\nmax_dens_a_prior <- max(c(priors$a_den, priors2$a_den))\r\n\r\n\r\n\r\nGetting the plot ranges\r\n\r\n\r\nrange_a0 <- c(\r\n  min(min(postdraws_vweak_prior$a0), min(postdraws_vweak_prior$a0)), \r\n  max(max(postdraws_vweak_prior$a0), max(postdraws_vweak_prior$a0))\r\n)\r\nrange_a1 <- c(\r\n  min(min(postdraws_vweak_prior$a1), min(postdraws_vweak_prior$a1)), \r\n  max(max(postdraws_vweak_prior$a1), max(postdraws_vweak_prior$a1))\r\n)\r\n\r\n\r\n\r\n7.2. Fig.2, Subfigure 1\r\n\r\n\r\nf2.s1.panC <-\r\n  ggplot(data = postdraws_vweak_prior, aes(x = a0, y = a1)) +\r\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\", colour = \"white\") +\r\n  annotate(\"point\", x = 1, y = 2, color = \"black\", size = 2, shape = 15) +\r\n  annotate(\"text\", x = 1.4, y = 1.9, label = \"Truth (1, 2)\", hjust = 0, color = \"black\", size = 3) +\r\n  annotate(\"point\",\r\n    x = mean(postdraws_vweak_prior$a0), y = mean(postdraws_vweak_prior$a1),\r\n    color = \"black\", size = 2, shape = 17\r\n  ) +\r\n  annotate(\"text\",\r\n    x = -5, y = 10,\r\n    label = paste0(\r\n      \"Posterior Mean\\n(\",\r\n      round(mean(postdraws_vweak_prior$a0), 1),\r\n      \", \",\r\n      round(mean(postdraws_vweak_prior$a1), 1),\r\n      \")\"\r\n    ),\r\n    hjust = 0.5, color = \"black\", size = 3\r\n  ) +\r\n  annotate(\"segment\",\r\n    x = mean(postdraws_vweak_prior$a0), xend = -5,\r\n    y = mean(postdraws_vweak_prior$a1), yend = 8.5\r\n  ) +\r\n  annotate(\"point\", x = 0, y = 0, color = \"black\", size = 2, shape = 15) +\r\n  annotate(\"text\", x = 0.4, y = -0.1, label = \"H0 (0, 0)\", hjust = 0, color = \"black\", size = 3) +\r\n  scale_x_continuous(expand = c(0, 0), limits = range_a0) +\r\n  scale_y_continuous(expand = c(0, 0), limits = range_a1) +\r\n  scale_fill_continuous(low = \"grey95\", high = \"grey30\") +\r\n  theme(legend.position = \"none\") +\r\n  labs(\r\n    x = expression(a[0]),\r\n    y = expression(a[1]),\r\n    subtitle = quote(\"Posterior: Pr(\" * a[0] * \",\" * a[1] * \" | y, x)\")\r\n  )\r\nf2.s1.panA <-\r\n  ggplot(data = priors, aes(x = x1, y = a_den)) +\r\n  geom_area(color = \"black\", fill = \"grey30\", alpha = 0.5) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.001, 0.05)), limits = c(0, max_dens_a_prior) ) +\r\n  annotate(\"text\", x = 290, y = 0.095, label = \"N(0, 100)\", hjust = 1, color = \"black\", size = 3) +\r\n  scale_x_continuous(expand = c(0, 0), breaks = c(-300, -150, 0, 150, 300)) +\r\n  labs(\r\n    y = \"Density\",\r\n    x = expression(a[k]),\r\n    subtitle = quote(\"Prior: Pr(\" * a[k] * \")\")\r\n  )\r\nf2.s1.panB <-\r\n  ggplot(data = priors, aes(x = x2, y = sigma_den)) +\r\n  geom_area(color = \"black\", fill = \"grey30\", alpha = 0.5) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.01, 0.05))) +\r\n  scale_x_continuous(expand = c(0, 0)) +\r\n  annotate(\"text\", x = 99, y = 0.045, label = \"Exp(1/21)\", hjust = 1, color = \"black\", size = 3) +\r\n  labs(\r\n    y = \"Density\",\r\n    x = expression(sigma),\r\n    subtitle = quote(\"Prior: Pr(\" * sigma * \")\")\r\n  )\r\nf2.s1.panD <-\r\n  post_dens %>%\r\n  ggplot(aes(x = x)) +\r\n  geom_ribbon(aes(ymin = 0, ymax = y, fill = quant), alpha = 0.7) +\r\n  geom_line(aes(y = y), color = \"black\") +\r\n  labs(\r\n    x = expression(a[1]),\r\n    y = expression(\"Density\"),\r\n    subtitle = quote(\"Posterior: Pr(\" * a[1] * \" | y, x)\")\r\n  ) +\r\n  scale_x_continuous(expand = c(0, 0)) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.0, 0.05))) +\r\n  scale_fill_manual(values = c(\"grey50\", \"grey70\", \"grey50\")) +\r\n  theme(legend.position = \"None\") +\r\n  annotate(\"segment\", x = 7, xend = 12, y = 0.02, yend = 0.05) +\r\n  annotate(\"text\", x = 11.5, y = 0.052, label = \"90% Credible\\ninterval\", color = \"black\", size = 3, hjust = 0) +\r\n  coord_flip(xlim = range_a1, ylim = c(0, max_dens))\r\n\r\nlayout <- \"\r\nACCD\r\nBCCD\r\n\"\r\n\r\nfig2.s1 <- f2.s1.panA + f2.s1.panB + f2.s1.panC + f2.s1.panD +\r\n  plot_layout(design = layout) +\r\n  plot_annotation(tag_levels = \"A\")\r\n\r\n\r\n\r\n7.3. Fig.2, Subfigure 2\r\n\r\n\r\nf2.s2.panC <-\r\n  ggplot(data = postdraws_wkinfo_prior, aes(x = a0, y = a1)) +\r\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\", colour = \"white\") +\r\n  annotate(\"point\", x = 1, y = 2, color = \"black\", size = 2, shape = 15) +\r\n  annotate(\"text\", x = 1.4, y = 1.9, label = \"Truth (1, 2)\", hjust = 0, color = \"black\", size = 3) +\r\n  annotate(\"point\",\r\n    x = mean(postdraws_wkinfo_prior$a0), y = mean(postdraws_wkinfo_prior$a1),\r\n    color = \"black\", size = 2, shape = 17\r\n  ) +\r\n  annotate(\"text\",\r\n    x = -5, y = 10,\r\n    label = paste0(\r\n      \"Posterior Mean\\n(\",\r\n      round(mean(postdraws_wkinfo_prior$a0), 1),\r\n      \", \",\r\n      round(mean(postdraws_wkinfo_prior$a1), 1),\r\n      \")\"\r\n    ),\r\n    hjust = 0.5, color = \"black\", size = 3\r\n  ) +\r\n  annotate(\"segment\",\r\n    x = mean(postdraws_wkinfo_prior$a0), xend = -5,\r\n    y = mean(postdraws_wkinfo_prior$a1), yend = 8.5\r\n  ) +\r\n  annotate(\"point\", x = 0, y = 0, color = \"black\", size = 2, shape = 15) +\r\n  annotate(\"text\", x = 0.4, y = -0.1, label = \"H0 (0, 0)\", hjust = 0, color = \"black\", size = 3) +\r\n  scale_x_continuous(expand = c(0, 0), limits = range_a0) +\r\n  scale_y_continuous(expand = c(0, 0), limits = range_a1) +\r\n  scale_fill_continuous(low = \"grey95\", high = \"grey30\") +\r\n  theme(legend.position = \"none\") +\r\n  labs(\r\n    x = expression(a[0]),\r\n    y = expression(a[1]),\r\n    subtitle = quote(\"Posterior: Pr(\" * a[0] * \",\" * a[1] * \" | y, x)\")\r\n  )\r\nf2.s2.panA <-\r\n  ggplot(data = priors2, aes(x = x1, y = a_den)) +\r\n  geom_area(color = \"black\", fill = \"grey30\", alpha = 0.5) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.001, 0.05)), limits = c(0, max_dens_a_prior)) +\r\n  annotate(\"text\", x = 290, y = 0.095, label = \"N(0, 4)\", hjust = 1, color = \"black\", size = 3) +\r\n  scale_x_continuous(expand = c(0, 0), breaks = c(-300, -150, 0, 150, 300)) +\r\n  labs(\r\n    y = \"Density\",\r\n    x = expression(a[k]),\r\n    subtitle = quote(\"Prior: Pr(\" * a[1] * \")\")\r\n  )\r\nf2.s2.panB <-\r\n  ggplot(data = priors2, aes(x = x2, y = sigma_den)) +\r\n  geom_area(color = \"black\", fill = \"grey30\", alpha = 0.5) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.01, 0.05))) +\r\n  scale_x_continuous(expand = c(0, 0)) +\r\n  annotate(\"text\", x = 99, y = 0.045, label = \"Exp(1/21)\", hjust = 1, color = \"black\", size = 3) +\r\n  labs(\r\n    y = \"Density\",\r\n    x = expression(sigma),\r\n    subtitle = quote(\"Prior: Pr(\" * sigma * \")\")\r\n  )\r\nf2.s2.panD <-\r\n  post_dens2 %>%\r\n  ggplot(aes(x = x)) +\r\n  geom_ribbon(aes(ymin = 0, ymax = y, fill = quant), alpha = 0.7) +\r\n  geom_line(aes(y = y), color = \"black\") +\r\n  labs(\r\n    x = expression(a[1]),\r\n    y = expression(\"Density\"),\r\n    subtitle = quote(\"Posterior: Pr(\" * a[1] * \" | y, x)\")\r\n  ) +\r\n  scale_x_continuous(expand = c(0, 0), limits = range_a1) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.0, 0.05))) +\r\n  scale_fill_manual(values = c(\"grey50\", \"grey70\", \"grey50\")) +\r\n  theme(legend.position = \"None\") +\r\n  annotate(\"segment\", x = 7, xend = 12, y = 0.02, yend = 0.05) +\r\n  annotate(\"text\", x = 11.5, y = 0.052, label = \"90% Credible\\ninterval\", color = \"black\", size = 3, hjust = 0) +\r\n  coord_flip(xlim = range_a1, ylim = c(0, max_dens))\r\n\r\nlayout <- \"\r\nACCD\r\nBCCD\r\n\"\r\n\r\nfig2.s2 <- f2.s2.panA + f2.s2.panB + f2.s2.panC + f2.s2.panD +\r\n  plot_layout(design = layout) +\r\n  plot_annotation(tag_levels = \"A\")\r\n\r\n\r\n\r\n\r\n\r\nfig2.s1\r\n\r\n\r\n\r\n\r\n\r\n\r\nfig2.s2\r\n\r\n\r\n\r\n\r\nSaving subfigures\r\n\r\n\r\nsave_fig(fig=fig2.s1, figname = \"fig2-1\", w = 6.2, h = 3.3)\r\nsave_fig(fig=fig2.s2, figname = \"fig2-2\", w = 6.2, h = 3.3)\r\n\r\n\r\n\r\n8. Fig. 3: Priors vs Likelihood\r\n8.1. Generate a larger sample\r\n\r\n\r\nset.seed(999)\r\nnew_obs <- 150\r\nx_fix2 <- rnorm(n = new_obs, 0, 1)\r\nsample2 <- tibble(u = rnorm(new_obs, 0, 20), x = x_fix2, y = 1 + 2 * x_fix2 + u)\r\nsample2 <- bind_rows(sample1, sample2)\r\nn_obs2 <- new_obs + n_obs\r\n\r\ninput_data2 <- list(\r\n  y = sample2$y,\r\n  x = sample2$x,\r\n  N = nrow(sample2)\r\n)\r\n\r\n\r\n\r\n8.2. Fitting the weakly informative model to larger sample\r\n\r\n\r\nfit_wkinfo_priors_large <- model_wkinfo_priors$sample(\r\n  data = input_data2,\r\n  iter_sampling = 1000,\r\n  iter_warmup = 1000,\r\n  chains = 4,\r\n  parallel_chains = 4,\r\n  seed = 1234,\r\n  refresh = 1000\r\n)\r\n\r\n\r\nRunning MCMC with 4 parallel chains...\r\n\r\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 1 finished in 0.1 seconds.\r\nChain 2 finished in 0.1 seconds.\r\nChain 3 finished in 0.1 seconds.\r\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 4 finished in 0.1 seconds.\r\n\r\nAll 4 chains finished successfully.\r\nMean chain execution time: 0.1 seconds.\r\nTotal execution time: 0.4 seconds.\r\n\r\n\r\n\r\npostdraws_wkinfo_prior_large <- fit_wkinfo_priors_large$draws(format = \"draws_df\")\r\nfit_wkinfo_priors_large$summary(variables = c(\"sigma\", \"a0\", \"a1\")) |> \r\n  mutate(across(where(is.numeric), round, 3))  |> \r\n  kable()\r\n\r\n\r\nvariable\r\nmean\r\nmedian\r\nsd\r\nmad\r\nq5\r\nq95\r\nrhat\r\ness_bulk\r\ness_tail\r\nsigma\r\n19.649\r\n19.617\r\n0.988\r\n1.000\r\n18.088\r\n21.313\r\n0.999\r\n4670.806\r\n2760.187\r\na0\r\n-0.136\r\n-0.149\r\n1.383\r\n1.358\r\n-2.424\r\n2.125\r\n1.001\r\n3777.942\r\n2872.741\r\na1\r\n2.467\r\n2.467\r\n1.324\r\n1.337\r\n0.317\r\n4.634\r\n1.000\r\n3953.647\r\n3027.360\r\n\r\n8.3. Collecting Prior, likelihood, posterior for small, current sample\r\n\r\n\r\nloglik_a1 <- function(a1, x, y, a0, u_sd) {\r\n  resids <- y - (a0 + a1 * x)\r\n  log_lik_vec <- dnorm(x = resids, mean = 0, sd = u_sd, log = TRUE)\r\n  log_lik <- sum(log_lik_vec)\r\n  return(log_lik)\r\n}\r\n\r\n\r\n\r\n\r\n\r\nols_model_small <- lm(y ~ x, data = sample1)\r\na0 <- ols_model_small$coefficients[[\"(Intercept)\"]]\r\nusd <- sd(ols_model_small$residuals)\r\na1_hat <- ols_model_small$coefficients[[\"x\"]]\r\n\r\nsmall_sample_data <- tibble(a1 = seq(-20, +30, length.out = 1000))\r\nsmall_sample_data$loglik <- map_dbl(\r\n  small_sample_data$a1,\r\n  loglik_a1,\r\n  x = sample1$x,\r\n  y = sample1$y,\r\n  a0 = a0,\r\n  u_sd = usd\r\n)\r\nsmall_sample_data <-\r\n  small_sample_data %>%\r\n  mutate(\r\n    # norm_lik = -2 * loglik, #exp(loglik) / sum(exp(loglik)),\r\n    norm_lik = (loglik),\r\n    prior = dnorm(a1, 0, 4),\r\n    post = approxfun(density(postdraws_wkinfo_prior$a1))(a1)\r\n  ) \r\nsmall_sample_data$post[is.na(small_sample_data$post)] <- 0\r\n\r\n\r\n\r\n8.4. Collecting Prior, likelihood, posterior for larger sample\r\n\r\n\r\nols_model_large <- lm(y ~ x, data = sample2)\r\na0 <- ols_model_large$coefficients[[\"(Intercept)\"]]\r\nusd <- sd(ols_model_large$residuals)\r\na1_hat_big <- ols_model_large$coefficients[[\"x\"]]\r\n\r\nlarge_sample_data <- tibble(a1 = seq(-20, +30, length.out = 1000))\r\nlarge_sample_data$loglik <- map_dbl(\r\n  large_sample_data$a1,\r\n  loglik_a1,\r\n  x = sample2$x,\r\n  y = sample2$y,\r\n  a0 = a0,\r\n  u_sd = usd\r\n)\r\nlarge_sample_data <-\r\n  large_sample_data %>%\r\n  mutate(\r\n    # norm_lik = -2 * loglik, #exp(loglik) / sum(exp(loglik)),\r\n    norm_lik = (loglik),\r\n    prior = dnorm(a1, 0, 4),\r\n    post = approxfun(density(postdraws_wkinfo_prior_large$a1))(a1)\r\n  ) \r\nlarge_sample_data$post[is.na(large_sample_data$post)] <- 0\r\n\r\n\r\n\r\n8.5. Fig. 3, Panel A and Panel B\r\n\r\n\r\nSCALER_1 <- -2 * 0.0001\r\nf3.panA <- \r\n  small_sample_data %>%\r\n  mutate(norm_lik = (-1600 - norm_lik) * SCALER_1) %>%\r\n  pivot_longer(c(norm_lik, prior, post), names_to = \"part\") %>%\r\n  ggplot(aes(x = a1, y = value, group = part, color = part, fill = part)) +\r\n  geom_ribbon(aes(ymin = 0, ymax = value), alpha = 0.5) +\r\n  geom_line() + \r\n  geom_vline(xintercept = a1_hat, color = \"grey80\", linetype = \"dashed\") +\r\n  labs(\r\n    y = \"Density\",\r\n    x = expression(a[1]),\r\n    subtitle = \"Small sample (n = 50)\"\r\n  ) +\r\n  annotate(\"text\",\r\n    x = c(-10, -4, 17),\r\n    y = c(0.1, 0.2, 0.20),\r\n    label = c(\"Prior\", \"Posterior\", \"Log likelihood\"),\r\n    hjust = 0.5,\r\n    size = 3\r\n  ) +\r\n  annotate(\"segment\",\r\n    x = c(-10, -4, 17),\r\n    y = c(0.090, 0.189, 0.21),\r\n    xend = c(-3.8, 2.5, 10),\r\n    yend = c(0.050, 0.13, 0.27),\r\n    color = c(\"grey50\", \"grey30\", \"grey80\")\r\n  ) +\r\n  annotate(\"text\",\r\n    x = a1_hat + 0.1,\r\n    y = 0.31, label = paste(\"Max likelihood at\", round(a1_hat, 1)),\r\n    hjust = 0, color = \"grey50\", size = 3\r\n  )\r\n  \r\n\r\nf3.panB <- \r\n  large_sample_data %>%\r\n  mutate(norm_lik = (-1600 - norm_lik) * SCALER_1) %>%\r\n  pivot_longer(c(norm_lik, prior, post), names_to = \"part\") %>%\r\n  ggplot(aes(x = a1, y = value, group = part, color = part, fill = part)) +\r\n  geom_ribbon(aes(ymin = 0, ymax = value), alpha = 0.5) +\r\n  geom_line() +\r\n  geom_vline(xintercept = a1_hat_big, color = \"grey80\", linetype = \"dashed\") +\r\n  labs(\r\n    y = \"Density\",\r\n    x = expression(a[1]),\r\n    subtitle = \"Large Sample (n = 200)\"\r\n  ) +\r\n  annotate(\"text\",\r\n    x = c(-10, -6, 17),\r\n    y = c(0.1, 0.2, 0.20),\r\n    label = c(\"Prior\", \"Posterior\", \"Log likelihood\"),\r\n    hjust = 0.5,\r\n    size = 3\r\n  ) +\r\n  annotate(\"segment\",\r\n    x = c(-10, -6, 17),\r\n    y = c(0.090, 0.189, 0.19),\r\n    xend = c(-3.8, 1.5, 10),\r\n    yend = c(0.050, 0.13, 0.13),\r\n    color = c(\"grey50\", \"grey50\", \"grey80\")\r\n  ) +\r\n  annotate(\"text\",\r\n    x = a1_hat_big + .1,\r\n    y = 0.31, label = paste(\"Max likelihood at\", round(a1_hat_big, 1)),\r\n    hjust = 0, color = \"grey50\", size = 3\r\n  ) \r\n\r\nfig3 <- \r\n  f3.panA + f3.panB +\r\n  plot_annotation(tag_levels = \"A\") &\r\n  annotate(\"segment\", x = 2, y = 0, xend = 2, yend = 0.02) &\r\n  theme(legend.position = \"none\", aspect.ratio=1) & \r\n  scale_color_manual(values = c(\"grey80\", \"grey30\", \"grey50\")) &\r\n  scale_fill_manual(values = c(\"grey80\", \"grey30\", \"grey50\")) &\r\n  scale_y_continuous(\r\n    expand = expansion(mult = c(0, 0.05)),\r\n    limits = c(0, 0.32),\r\n    sec.axis = sec_axis(~ -1600 - . / SCALER_1, name = \"Log likelihood\")\r\n  ) &\r\n  scale_x_continuous(\r\n    breaks = c(-20, -10, 0, 2, 10, 20, 30),\r\n    expand = expansion(mult = c(0, 0))\r\n  )\r\nfig3\r\n\r\n\r\n\r\n\r\nSaving figure\r\n\r\n\r\nsave_fig(fig=fig3, figname = \"fig3\", w = 6.2, h = 3.1)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-10-03T17:16:08+02:00"
    },
    {
      "path": "ERC-example-prior.html",
      "title": "Prior Predictive Check for the Multi-level ERC Model",
      "description": "Part two of the code for Section 3: Heterogeneity in earnings response coefficients\n",
      "author": [
        {
          "name": "Harm Schuett",
          "url": "https://hschuett.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\n1. Introduction and preliminaries\r\n2. Loading the data\r\n3. Prior predictive check\r\n\r\n\r\n\r\n\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(cmdstanr)\r\nlibrary(posterior)\r\nkable <- knitr::kable\r\nsource(\"00-utils.R\")\r\n\r\n\r\n\r\n1. Introduction and preliminaries\r\nThis markdown file contains all the code necessary to replicate the prior predictive check in Section 3: Heterogeneity in earnings response coefficients of the Paper What Can Bayesian Inference Do for Accounting Research?. All the code can also be found in the repo. It contains 00-utils.R which contains a few helper functions for graphs and tables.\r\nNote: I used the newer cmdstanr package instead of the older rstan package because it likely is the future of the R based Stan ecosystem. I also really like its api, which is very close to the api of the pystan package. An additional advantage (I hope) is thus that most model fitting code should be more or less directly transferable to pystan for those that want to work in python. Installing cmdstanr used to be tricky at times because one needs a working c++ toolchain. But it is much smoother now. Please see the cmdstanr doc for installation instructions\r\n2. Loading the data\r\nSame as in the main part of the ERC example. This is just de-meaning the y- and x-variable.\r\n\r\n\r\nea_data <- arrow::read_parquet(\"../data/ea-event-returns.pqt\") \r\nea_data <- \r\n  ea_data |> \r\n  mutate(\r\n    ret_dm = AbEvRet - mean(AbEvRet),\r\n    earn_surp_dm = earn_surp - mean(earn_surp)\r\n    )\r\nhead(ea_data)\r\n\r\n\r\n# A tibble: 6 x 15\r\n  ticker permno fpend_date ea_date    actual_eps median_fcast_eps\r\n  <chr>   <dbl> <date>     <date>          <dbl>            <dbl>\r\n1 001N    14504 2014-12-31 2015-02-09       0.04            0.03 \r\n2 001N    14504 2015-03-31 2015-05-05       0.06           -0.045\r\n3 001N    14504 2015-06-30 2015-08-05       0.02           -0.015\r\n4 001N    14504 2015-09-30 2015-11-04      -0.02            0.04 \r\n5 002T    14503 2014-06-30 2014-08-07       0.19            0.235\r\n6 002T    14503 2014-09-30 2014-11-04       0.18            0.34 \r\n# ... with 9 more variables: num_forecasts <int>,\r\n#   two_days_bef_ea <date>, Price <dbl>, earn_surp <dbl>,\r\n#   ea_match_date <date>, AbEvRet <dbl>, firm_id <int>, ret_dm <dbl>,\r\n#   earn_surp_dm <dbl>\r\n\r\n3. Prior predictive check\r\nBefore fitting the actual model, I want to check whether the chosen priors are sort of sensible. To this end I wrote a Stan model that does not actually fit the model but simulates data from the chosen priors. It might be easier to skip this for now, continue to the actual model fitting part and then come back to review the prior predictive checking afterwards.\r\n\r\n\r\ncat(read_lines(\"../Stan/erc-wkinfo-priorpred.stan\"), sep = \"\\n\")\r\n\r\n\r\ndata{\r\n  int<lower=1> N;                   // num obs\r\n  int<lower=1> J;                   // num groups\r\n  int<lower=1> K;                   // num coefficients\r\n  int<lower=1, upper=J> GroupID[N]; // GroupID for obs, e.g. FirmID or Industry-YearID\r\n  // vector[N] y;                      // Response\r\n  matrix[N, K] x;                   // Predictors (incl. Intercept)\r\n}\r\ngenerated quantities {\r\n  matrix[K, J] z;                  // standard normal sampler\r\n  cholesky_factor_corr[K] L_Omega; // hypprior coefficient correlation\r\n  vector<lower=0>[K] tau;          // hypprior coefficient scales\r\n  vector[K] mu_b;                  // hypprior mean coefficients\r\n  real<lower=0> sigma;             // error-term scale\r\n  matrix[J, K] b;                  // coefficient vector\r\n  array[N] real y_pred;\r\n\r\n  for (k in 1:K) {\r\n    for (j in 1:J){\r\n      z[k, j] = normal_rng(0, 1);\r\n    }\r\n  }\r\n  L_Omega = lkj_corr_cholesky_rng(K, 2);\r\n  mu_b[1] = normal_rng(0, 0.1);\r\n  mu_b[2] = normal_rng(0, 40);\r\n  sigma = exponential_rng(1.0 / 0.08);   // exp: 0.08 (std (abnormal returns))\r\n  tau[1] = exponential_rng(1.0 / 0.1);   // exp: 0.1\r\n  tau[2] = exponential_rng(1.0 / 40);    // exp: 40\r\n  b = (rep_matrix(mu_b, J) + diag_pre_multiply(tau,L_Omega) * z)';\r\n  y_pred = normal_rng(rows_dot_product(b[GroupID] , x), sigma);\r\n}\r\n\r\nIn contrast to normal models, this one has no model block. Just generated quantities\r\n\r\n\r\nmodel_priorpred <- cmdstan_model(\"../Stan/erc-wkinfo-priorpred.stan\")\r\nprio_check <- model_priorpred$sample(\r\n  data = list(\r\n    N = nrow(ea_data),\r\n    J = max(ea_data$firm_id),\r\n    K = 2,\r\n    GroupID = ea_data$firm_id,\r\n    x = as.matrix(data.frame(int = 1, esurp = ea_data$earn_surp))\r\n  ),\r\n  iter_sampling = 1000,\r\n  iter_warmup = 0,\r\n  seed = 1234,\r\n  refresh = 1000,\r\n  fixed_param = TRUE  \r\n)\r\n\r\n\r\nRunning MCMC with 1 chain...\r\n\r\nChain 1 Iteration:   1 / 1000 [  0%]  (Sampling) \r\nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \r\nChain 1 finished in 94.5 seconds.\r\n\r\nThe fixed_param = TRUE option is necessary to invoke a different sampler. From the docs: “The fixed parameter sampler generates a new sample without changing the current state of the Markov chain; only generated quantities may change”\r\nThe next piece collects the draws for key parameters and computes the descriptive stats for Panel B of Table 1\r\n\r\n\r\nmu_a1_prip <- prio_check$draws(variables= c(\"mu_b[2]\"), format = \"matrix\")\r\nsig_a1_prip <- prio_check$draws(variables= c(\"tau[2]\"), format = \"matrix\")\r\na_prip <- prio_check$draws(variables= c(\"b\"), format = \"matrix\")\r\na1_prip <- as.vector(a_prip[, grepl(\",2\\\\]\", colnames(a_prip))])\r\ny_prip <- \r\n  prio_check$draws(variables= c(\"y_pred\"), format = \"list\") %>% \r\n  unlist()\r\n\r\n\r\n\r\n\r\n\r\ntab1.B <- rbind(\r\n  desc_row(mu_a1_prip, \"mu_a1_prip\"),\r\n  desc_row(sig_a1_prip, \"sig_a1_prip\"),\r\n  desc_row(a1_prip, \"a_1_prip\"),\r\n  desc_row((y_prip + mean(ea_data$AbEvRet)), \"Ret_prip\")\r\n  )\r\n\r\nwrite_csv(tab1.B, \"../out/results/tab1-panB.csv\")\r\n\r\n\r\n\r\n\r\n\r\nkable(tab1.B)\r\n\r\n\r\n\r\nvar\r\nmean\r\nsd\r\nq5\r\nq25\r\nq50\r\nq75\r\nq95\r\n5%\r\nmu_a1_prip\r\n1.0676335\r\n39.283343\r\n-63.1713500\r\n-23.7874250\r\n1.1804850\r\n28.5151750\r\n62.1924700\r\n5%1\r\nsig_a1_prip\r\n38.4404197\r\n38.405202\r\n1.9138375\r\n10.8653250\r\n27.3052500\r\n52.8953250\r\n117.8279000\r\n5%2\r\na_1_prip\r\n1.0642062\r\n67.046823\r\n-101.4470000\r\n-35.1546250\r\n1.8402750\r\n37.2140000\r\n100.4770000\r\n5%3\r\nRet_prip\r\n0.0011037\r\n0.363066\r\n-0.4706719\r\n-0.1396009\r\n0.0017968\r\n0.1422191\r\n0.4687342\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-10-03T18:48:31+02:00"
    },
    {
      "path": "ERC-example.html",
      "title": "An example of Bayesian modelling: Firm-level ERC coefficients",
      "description": "Part one of the code for Section 3: Heterogeneity in earnings response coefficients\n",
      "author": [
        {
          "name": "Harm Schuett",
          "url": "https://hschuett.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\n1. Introduction and preliminaries\r\n2. Loading the data\r\n2.1. Final transformations\r\n2.2. Tab. 1, Panel A.\r\n\r\n3. OLS estimates\r\n3.1 Pooled OLS estimates\r\n3.2 Firm-level OLS estimates\r\n3.3 By-year OLS estimates\r\n\r\n4. Bayesian model with weakly informative priors\r\n4.1 Model fitting\r\n4.2 Summary of the posterior for selected parameters\r\n4.3 Comparing Bayesian ERC estimates to OLS estimates\r\n4.4 Figure 4\r\n\r\n5. Bayesian model with AR1 time trend\r\n5.1 Model fitting\r\n5.2 Summary of the posterior for selected parameters\r\n5.3 Comparing Bayesian ERC estimates to OLS estimates\r\n5.4 Fig. 5\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(cmdstanr)\r\nlibrary(posterior)\r\nlibrary(patchwork)\r\nsource(\"00-utils.R\")\r\nkable <- knitr::kable\r\ntheme_set(theme_prodgray())\r\n\r\nols_erc <- function(.d){\r\n  fit <- lm(ret_dm ~ earn_surp_dm, data = .d)\r\n  fit_results <- broom::tidy(fit)\r\n}\r\n\r\nols_pred <- function(.d){\r\n  fit <- lm(ret_dm ~ earn_surp_dm, data = .d)\r\n  fit_pred <- broom::augment(fit)\r\n}\r\n\r\n\r\n\r\n1. Introduction and preliminaries\r\nThis markdown file contains all the code necessary to replicate the main figures, models and results used in Section 3: Heterogeneity in earnings response coefficients of the Paper What Can Bayesian Inference Do for Accounting Research?. The prior predictive checks and the cross-validation test are on separate pages. All the code can also be found in the repo. It contains 00-utils.R which contains a few helper functions for graphs and tables.\r\nNote: I used the newer cmdstanr package instead of the older rstan package because it likely is the future of the R based Stan ecosystem. I also really like its api, which is very close to the api of the pystan package. An additional advantage (I hope) is thus that most model fitting code should be more or less directly transferable to pystan for those that want to work in python. Installing cmdstanr used to be tricky at times because one needs a working c++ toolchain. But it is much smoother now. Please see the cmdstanr doc for installation instructions\r\n2. Loading the data\r\n2.1. Final transformations\r\nThe data used here is generated via the 02-create-ERC-sample.R script found in the repo. Here, we just load it and do some last minute transformations like de-meaning, etc.\r\n\r\n\r\nea_data <- arrow::read_parquet(\"../data/ea-event-returns.pqt\") \r\nea_data <- \r\n  ea_data |> \r\n  mutate(\r\n    ret_dm = AbEvRet - mean(AbEvRet),\r\n    earn_surp_dm = earn_surp - mean(earn_surp)\r\n    )\r\nhead(ea_data) |> \r\n  kable()\r\n\r\n\r\nticker\r\npermno\r\nfpend_date\r\nea_date\r\nactual_eps\r\nmedian_fcast_eps\r\nnum_forecasts\r\ntwo_days_bef_ea\r\nPrice\r\nearn_surp\r\nea_match_date\r\nAbEvRet\r\nfirm_id\r\nret_dm\r\nearn_surp_dm\r\n001N\r\n14504\r\n2014-12-31\r\n2015-02-09\r\n0.04\r\n0.030\r\n3\r\n2015-02-05\r\n14.49\r\n0.0006901\r\n2015-02-09\r\n-0.2428478\r\n1\r\n-0.2472159\r\n0.0006950\r\n001N\r\n14504\r\n2015-03-31\r\n2015-05-05\r\n0.06\r\n-0.045\r\n4\r\n2015-05-01\r\n12.42\r\n0.0084541\r\n2015-05-05\r\n-0.0452097\r\n1\r\n-0.0495779\r\n0.0084589\r\n001N\r\n14504\r\n2015-06-30\r\n2015-08-05\r\n0.02\r\n-0.015\r\n4\r\n2015-08-03\r\n9.25\r\n0.0037838\r\n2015-08-05\r\n0.0469699\r\n1\r\n0.0426018\r\n0.0037886\r\n001N\r\n14504\r\n2015-09-30\r\n2015-11-04\r\n-0.02\r\n0.040\r\n5\r\n2015-11-02\r\n5.82\r\n-0.0103093\r\n2015-11-04\r\n0.0769411\r\n1\r\n0.0725729\r\n-0.0103044\r\n002T\r\n14503\r\n2014-06-30\r\n2014-08-07\r\n0.19\r\n0.235\r\n4\r\n2014-08-05\r\n33.33\r\n-0.0013501\r\n2014-08-07\r\n-0.0166244\r\n2\r\n-0.0209926\r\n-0.0013453\r\n002T\r\n14503\r\n2014-09-30\r\n2014-11-04\r\n0.18\r\n0.340\r\n5\r\n2014-10-31\r\n30.43\r\n-0.0052580\r\n2014-11-04\r\n-0.0211484\r\n2\r\n-0.0255165\r\n-0.0052531\r\n\r\n\r\n\r\nea_data_time <- \r\n  ea_data |> \r\n  mutate(EAYear = lubridate::year(ea_date)) |> \r\n  mutate(year_id = as.integer(EAYear - min(EAYear) + 1))\r\n\r\n\r\n\r\n2.2. Tab. 1, Panel A.\r\n\r\n\r\ndesc_tabl <- rbind(\r\n  desc_row(ea_data$AbEvRet, \"Ret\"),\r\n  desc_row(ea_data$earn_surp, \"X\")\r\n)\r\ndesc_tabl$N <- nrow(ea_data)\r\ndesc_tabl$Firms <- max(ea_data$firm_id)\r\n\r\nwrite_csv(desc_tabl, \"../out/results/tab1-panA.csv\")\r\n\r\ndesc_tabl|> \r\n  mutate(across(where(is.numeric), round, 4)) |> \r\n  kable()\r\n\r\n\r\n\r\nvar\r\nmean\r\nsd\r\nq5\r\nq25\r\nq50\r\nq75\r\nq95\r\nN\r\nFirms\r\n5%\r\nRet\r\n0.0044\r\n0.0779\r\n-0.1252\r\n-0.0394\r\n0.0027\r\n0.0470\r\n0.1392\r\n67360\r\n2966\r\n5%1\r\nX\r\n0.0000\r\n0.0044\r\n-0.0075\r\n-0.0005\r\n0.0003\r\n0.0014\r\n0.0056\r\n67360\r\n2966\r\n\r\n3. OLS estimates\r\n3.1 Pooled OLS estimates\r\nJust to have a frame of reference, here is the pooled ERC estimate\r\n\r\n\r\npooled <- ols_erc(ea_data)\r\nkable(pooled)\r\n\r\n\r\nterm\r\nestimate\r\nstd.error\r\nstatistic\r\np.value\r\n(Intercept)\r\n0.000000\r\n0.0002955\r\n0.00000\r\n1\r\nearn_surp_dm\r\n3.116373\r\n0.0665200\r\n46.84867\r\n0\r\n\r\n3.2 Firm-level OLS estimates\r\nNext, we nest the data by firm (ticker) and fit OLS ERC models by firm\r\n\r\n\r\nnested_data <-\r\n  ea_data %>%\r\n  add_count(ticker, name = \"n_EAs\") %>%\r\n  nest(data = -c(ticker, n_EAs, firm_id)) %>%\r\n  mutate(ols_result = map(.x = data, .f = ~ols_erc(.)))\r\n\r\nsummary(nested_data$n_EAs)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n   1.00    4.00   13.00   22.71   33.00  122.00 \r\n\r\n\r\n\r\nols_results <-\r\n  nested_data %>%\r\n  select(ticker, n_EAs, ols_result) %>%\r\n  unnest(ols_result) %>%\r\n  filter(term == \"earn_surp_dm\")\r\n\r\nols_intercept <-\r\n  nested_data %>%\r\n  select(ticker, n_EAs, ols_result) %>%\r\n  unnest(ols_result) %>%\r\n  filter(term == \"(Intercept)\")\r\n\r\nwrite_csv(ols_results, \"../out/results/ols_bi.csv\")\r\n\r\n\r\n\r\nThis is the distribution of firm-level ERC estimates\r\n\r\n\r\nsummary(ols_results$estimate)\r\n\r\n\r\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's \r\n-5034.136     0.068     3.884     6.041    10.650  5462.750       294 \r\n\r\n3.3 By-year OLS estimates\r\nThis is for the comparison in Figure 5.\r\n\r\n\r\nbyyear_data <-\r\n  ea_data_time %>%\r\n  nest(data = -c(year_id, EAYear)) %>%\r\n  mutate(ols_result = map(.x = data, .f = ~ols_erc(.)))\r\n\r\n\r\n\r\n\r\n\r\nbyyear_results <-\r\n  byyear_data %>%\r\n  select(-data) %>%\r\n  unnest(ols_result) %>%\r\n  filter(term == \"earn_surp_dm\")\r\n\r\n\r\n\r\n4. Bayesian model with weakly informative priors\r\n4.1 Model fitting\r\nTo fit a Bayesian model I use Stan, or, more precisely, its R bindings in cmdstanr. To fit a Bayesian model, we need to:\r\nWrite the corresponding model using the Stan language\r\nCompile the code into a Stan model executable (an .exe file)\r\nMake a list of data to feed into the .exe file\r\nLet the model run and generate MCMC chains (or whatever algorithm is specified)\r\nThe model itself is coded in the Stan language. There are many excellent tutorials on Stan available online. So I won’t waste space explaining it here. For various reasons (e.g., debugging) it is customary to put the model code in a separate .stan file. All the model files can be found in the /Stan/ folder of the repo.\r\n\r\n\r\ncat(read_lines(\"../Stan/erc-wkinfo-priors.stan\"), sep = \"\\n\")\r\n\r\n\r\ndata{\r\n  int<lower=1> N;                   // num obs\r\n  int<lower=1> J;                   // num groups\r\n  int<lower=1> K;                   // num coefficients\r\n  int<lower=1, upper=J> GroupID[N]; // GroupID for obs, e.g. FirmID or Industry-YearID\r\n  vector[N] y;                      // Response\r\n  matrix[N, K] x;                   // Predictors (incl. Intercept)\r\n}\r\nparameters{\r\n  matrix[K, J] z;                  // standard normal sampler\r\n  cholesky_factor_corr[K] L_Omega; // hypprior coefficient correlation\r\n  vector<lower=0>[K] tau;          // hypprior coefficient scales\r\n  vector[K] mu_b;                  // hypprior mean coefficients\r\n  real<lower=0> sigma;             // error-term scale\r\n}\r\ntransformed parameters{\r\n  matrix[J, K] b;                  // coefficient vector\r\n  // The multivariate non-centered version:\r\n  b = (rep_matrix(mu_b, J) + diag_pre_multiply(tau,L_Omega) * z)';\r\n}\r\nmodel{\r\n  to_vector(z) ~ normal(0, 1);\r\n  L_Omega ~ lkj_corr_cholesky(2);\r\n  mu_b[1]  ~ normal(0, 0.1);\r\n  mu_b[2]  ~ normal(0, 40);\r\n  sigma ~ exponential(1.0 / 0.08);   // exp: 0.08 (std (abnormal returns))\r\n  tau[1] ~ exponential(1.0 / 0.1);   // exp: 0.1\r\n  tau[2] ~ exponential(1.0 / 40);    // exp: 40\r\n  y ~ normal(rows_dot_product(b[GroupID] , x), sigma);\r\n}\r\n// generated quantities {\r\n//   array[N] real y_pred = normal_rng(rows_dot_product(b[GroupID] , x), sigma);\r\n// }\r\n\r\nThe model code in Stan language\r\nNext, we compile the model to an .exe file\r\n\r\n\r\nmodel_wkinfo_priors <- cmdstan_model(\"../Stan/erc-wkinfo-priors.stan\")\r\n\r\n\r\n\r\nNow, we prepare the list of data to feed into the model.\r\n\r\n\r\ninput_data <- list(\r\n  N = nrow(ea_data),\r\n  J = max(ea_data$firm_id),\r\n  K = 2,\r\n  GroupID = ea_data$firm_id,\r\n  y = ea_data$AbEvRet,\r\n  x = as.matrix(data.frame(int = 1, esurp = ea_data$earn_surp))\r\n)\r\n\r\n\r\n\r\nNote how the list matches the names of the variables declared in the model’s data block\r\nWe run the model\r\nBeware, this fit can take a long time\r\n\r\n\r\nfit_wkinfo_priors <- model_wkinfo_priors$sample(\r\n  data = input_data,\r\n  iter_sampling = 1000,\r\n  iter_warmup = 1000,\r\n  chains = 4,\r\n  parallel_chains = 4,\r\n  seed = 1234,\r\n  refresh = 1000\r\n)\r\n\r\n\r\nRunning MCMC with 4 parallel chains...\r\n\r\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 3 finished in 1941.6 seconds.\r\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 4 finished in 1962.7 seconds.\r\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 1 finished in 2088.8 seconds.\r\nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 2 finished in 2132.0 seconds.\r\n\r\nAll 4 chains finished successfully.\r\nMean chain execution time: 2031.3 seconds.\r\nTotal execution time: 2132.6 seconds.\r\n\r\n4.2 Summary of the posterior for selected parameters\r\nHere is the summary of the resulting posterior distribution of the model parameters\r\n\r\n\r\nfit_wkinfo_priors$summary(variables = c(\"mu_b\", \"sigma\", \"tau\", \"L_Omega[2,1]\")) |> \r\n  mutate(across(where(is.numeric), round, 3)) |> \r\n  kable()\r\n\r\n\r\nvariable\r\nmean\r\nmedian\r\nsd\r\nmad\r\nq5\r\nq95\r\nrhat\r\ness_bulk\r\ness_tail\r\nmu_b[1]\r\n0.004\r\n0.004\r\n0.000\r\n0.000\r\n0.003\r\n0.004\r\n1.000\r\n3943.605\r\n3333.804\r\nmu_b[2]\r\n3.669\r\n3.668\r\n0.107\r\n0.108\r\n3.490\r\n3.850\r\n1.001\r\n2575.785\r\n2404.202\r\nsigma\r\n0.076\r\n0.076\r\n0.000\r\n0.000\r\n0.075\r\n0.076\r\n1.003\r\n4585.697\r\n2582.434\r\ntau[1]\r\n0.006\r\n0.006\r\n0.001\r\n0.001\r\n0.005\r\n0.007\r\n1.000\r\n1037.872\r\n2015.912\r\ntau[2]\r\n2.804\r\n2.802\r\n0.125\r\n0.125\r\n2.600\r\n3.008\r\n1.002\r\n1347.643\r\n2243.416\r\nL_Omega[2,1]\r\n-0.020\r\n-0.024\r\n0.088\r\n0.086\r\n-0.161\r\n0.130\r\n1.011\r\n276.749\r\n683.598\r\n\r\n\r\n\r\ntab1.D <- fit_wkinfo_priors$summary(\r\n  variables = c(\"mu_b\", \"sigma\", \"tau\", \"L_Omega[2,1]\"), \r\n  mean, sd, ~quantile2(., probs = c(0.05, 0.25, 0.5, 0.75, 0.95))\r\n)\r\nwrite_csv(tab1.D, \"../out/results/tab1-panD.csv\")\r\n\r\n\r\n\r\n4.3 Comparing Bayesian ERC estimates to OLS estimates\r\nTo do this we need to extract the posterior draws for the ERC coefficients\r\n\r\n\r\nposterior_b <- summarise_draws(fit_wkinfo_priors$draws(c(\"b\")), \r\n                               posterior_mean = mean, \r\n                               posterior_sd = sd,\r\n                               ~quantile2(., probs = c(0.05, 0.25, 0.75, 0.95))\r\n                               )\r\nwrite_csv(posterior_b, \"../out/results/fit_wkinfo_bi.csv\")\r\n\r\nposterior_erc <- \r\n  posterior_b |> \r\n  filter(str_detect(variable, \",2\\\\]\"))\r\n\r\n\r\n\r\nCode for Tab. 1, Panel C\r\n\r\n\r\ntab1.C <- \r\n  rbind(\r\n    desc_row(posterior_erc$posterior_mean, \"post_mean\"),\r\n    desc_row(with(posterior_erc, q95 - q5), \"post90_width\"),\r\n    desc_row(with(ols_results, estimate[is.na(estimate) == FALSE]), \"OLS\"),\r\n    desc_row(with(ols_results, p.value[is.na(p.value) == FALSE]), \"OLS pval\")\r\n  )\r\nwrite_csv(tab1.C, \"../out/results/tab1-panC.csv\")\r\ntab1.C |> \r\n  mutate(across(where(is.numeric), round, 3))\r\n\r\n\r\n             var  mean      sd      q5   q25   q50    q75    q95\r\n5%     post_mean 3.672   1.414   1.292 2.970 3.676  4.288  6.105\r\n5%1 post90_width 7.837   1.440   4.946 6.933 8.314  9.013  9.345\r\n5%2          OLS 6.041 178.593 -18.689 0.068 3.884 10.650 45.904\r\n5%3     OLS pval 0.362   0.309   0.002 0.068 0.301  0.611  0.927\r\n\r\n4.4 Figure 4\r\n\r\n\r\ngraph_data <-\r\n  ols_results %>%\r\n  mutate(\r\n    post_mean = posterior_erc$posterior_mean,\r\n    post_sd = posterior_erc$posterior_sd\r\n  )\r\n\r\n\r\n\r\n\r\n\r\ny_range <- c(-100, 100)\r\n\r\nf4.panA <-\r\n  graph_data %>%\r\n  ggplot(aes(x = n_EAs, y = estimate)) +\r\n  geom_point(alpha = 0.1, size = 1) + # ,  width = 0.25) +\r\n  geom_hline(yintercept = c(0)) +\r\n  geom_hline(yintercept = mean(graph_data$estimate, na.rm = TRUE), linetype = \"dashed\") +\r\n  labs(\r\n    y = \"OLS estimate\",\r\n    x = NULL, # \"Nr of Quarters in firm's time series\",\r\n    subtitle = \"OLS by firm\"\r\n  ) +\r\n  coord_cartesian(ylim = y_range)\r\n\r\nf4.panB <-\r\n  graph_data %>%\r\n  ggplot(aes(x = n_EAs, y = post_mean)) +\r\n  geom_point(alpha = 0.1, size = 1) + # ,  width = 0.25) +\r\n  geom_hline(yintercept = c(0)) +\r\n  geom_hline(yintercept = mean(graph_data$post_mean), linetype = \"dashed\") +\r\n  labs(\r\n    y = \"Posterior mean\",\r\n    x = NULL,\r\n    subtitle = \"Bayesian multi-level model\"\r\n  ) +\r\n  coord_cartesian(ylim = y_range)\r\n  \r\n\r\nf4.panC <-\r\n  graph_data %>%\r\n  ggplot(aes(x = n_EAs)) +\r\n  geom_bar(width = 0.5) +\r\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\r\n  labs(\r\n    x = \"Time series length\",\r\n    y = \"N. firms\"\r\n  )\r\n\r\nfig4 <- \r\n  f4.panA + f4.panB + f4.panC + f4.panC + \r\n  plot_layout(ncol = 2, heights = c(2, 0.5)) +\r\n  plot_annotation(tag_levels = list(c(\"A\", \"B\", NULL, NULL))) &\r\n  theme(legend.position = \"none\")\r\nfig4\r\n\r\n\r\n\r\n\r\nSaving figure\r\n\r\n\r\nsave_fig(fig4, figname = \"fig4\", w = 6.2, h = 3.4)\r\n\r\n\r\n\r\n5. Bayesian model with AR1 time trend\r\n5.1 Model fitting\r\n\r\n\r\ncat(read_lines(\"../Stan/erc-wkinfo-priors-time-pers.stan\"), sep = \"\\n\")\r\n\r\n\r\ndata{\r\n  int<lower=1> N;                   // num obs\r\n  int<lower=1> J;                   // num groups\r\n  int<lower=1> K;                   // num coefficients\r\n  int<lower=1> M;                   // num periods\r\n  int<lower=1, upper=J> GroupID[N]; // GroupID for obs, e.g. FirmID or Industry-YearID\r\n  int<lower=1, upper=M> TimeID[N];  // GroupID for obs, e.g. FirmID or Industry-YearID\r\n  vector[N] y;                      // Response\r\n  vector[N] x;                      // Predictor (without Intercept)\r\n}\r\nparameters{\r\n  matrix[K, J] z;                  // standard normal sampler\r\n  cholesky_factor_corr[K] L_Omega; // hypprior coefficient correlation\r\n  vector<lower=0>[K] tau;          // hypprior coefficient scales\r\n  real<lower=0> sigma;             // error-term scale\r\n  real mu_0;\r\n\r\n  real<lower=0,upper=1> rho_raw;   // used to construct rho, the AR(1) coefficient\r\n  real<lower=0> sig_t;             // error-term scale\r\n  vector[M] z_t;\r\n}\r\ntransformed parameters{\r\n  matrix[J, K] b_i;                // firm-level components\r\n  vector[K] mu_b;                  // hypprior mean firm-level coefficients\r\n  // The multivariate non-centered version:\r\n  mu_b[1] = mu_0;\r\n  mu_b[2] = 0;\r\n  b_i = (rep_matrix(mu_b, J) + diag_pre_multiply(tau,L_Omega) * z)';\r\n\r\n  // non-centered parameterization of AR(1) process priors\r\n  real rho = 2 * rho_raw - 1;      // ensures that rho is between -1 and 1\r\n  vector[M] b_t = sig_t * z_t;     // all of them share this term\r\n  b_t[1] /= sqrt(1 - rho^2);       // mo[1] = mo[1] / sqrt(1 - rho^2)\r\n  for (m in 2:M) {\r\n    b_t[m] += rho * b_t[m-1];      // mo[m] = mo[m] + rho * mo[m-1];\r\n  }\r\n\r\n}\r\nmodel{\r\n  to_vector(z) ~ normal(0, 1);\r\n  z_t ~ normal(0, 1);\r\n  L_Omega ~ lkj_corr_cholesky(2);\r\n  mu_0  ~ normal(0, 0.1);\r\n  rho_raw  ~ beta(10, 5);\r\n\r\n  sigma ~ exponential(1.0 / 0.08);   // exp: 0.08 (std (abnormal returns))\r\n  tau[1] ~ exponential(1.0 / 0.1);   // exp: 0.1\r\n  tau[2] ~ exponential(1.0 / 40);    // exp: 40\r\n  sig_t ~ exponential(1.0 / 40);     // exp: 40\r\n\r\n  vector[N] n_loc = b_i[GroupID, 1] + (b_t[TimeID] + b_i[GroupID, 2]) .* x;\r\n\r\n  y ~ normal(n_loc, sigma);\r\n}\r\n\r\n\r\n\r\ninput_data2 <- list(\r\n  N = nrow(ea_data_time),\r\n  J = max(ea_data_time$firm_id),\r\n  K = 2,\r\n  M = max(ea_data_time$year_id),\r\n  TimeID = ea_data_time$year_id, \r\n  GroupID = ea_data_time$firm_id,\r\n  y = ea_data$AbEvRet,\r\n  x = ea_data$earn_surp\r\n)\r\n\r\n\r\n\r\n\r\n\r\nmodel_wkinfo_timepers <- cmdstan_model(\"../Stan/erc-wkinfo-priors-time-pers.stan\")\r\n\r\n\r\n\r\nAgain beware, this fit can take a long time\r\n\r\n\r\nfit_wkinfo_timepers <- model_wkinfo_timepers$sample(\r\n  data = input_data2,\r\n  iter_sampling = 1000,\r\n  iter_warmup = 1000,\r\n  chains = 4,\r\n  parallel_chains = 4,\r\n  seed = 1234,\r\n  refresh = 1000\r\n)\r\n\r\n\r\nRunning MCMC with 4 parallel chains...\r\n\r\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \r\nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \r\nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \r\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 3 finished in 2301.3 seconds.\r\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 1 finished in 2386.3 seconds.\r\nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 2 finished in 2400.0 seconds.\r\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \r\nChain 4 finished in 2537.5 seconds.\r\n\r\nAll 4 chains finished successfully.\r\nMean chain execution time: 2406.3 seconds.\r\nTotal execution time: 2538.0 seconds.\r\n\r\n5.2 Summary of the posterior for selected parameters\r\nHere is the summary of the resulting posterior distribution of the model parameters\r\n\r\n\r\nfit_wkinfo_timepers$summary(variables = c(\"mu_b[1]\", \"sigma\", \"tau\", \"b_t\", \r\n                                          \"rho\", \"sig_t\", \"L_Omega[2,1]\")) |> \r\n  mutate(across(where(is.numeric), round, 3)) |> \r\n  kable()\r\n\r\n\r\nvariable\r\nmean\r\nmedian\r\nsd\r\nmad\r\nq5\r\nq95\r\nrhat\r\ness_bulk\r\ness_tail\r\nmu_b[1]\r\n0.003\r\n0.003\r\n0.000\r\n0.000\r\n0.003\r\n0.004\r\n1.004\r\n4608.601\r\n3102.494\r\nsigma\r\n0.075\r\n0.075\r\n0.000\r\n0.000\r\n0.075\r\n0.076\r\n1.000\r\n3703.130\r\n2184.654\r\ntau[1]\r\n0.006\r\n0.006\r\n0.001\r\n0.001\r\n0.005\r\n0.007\r\n1.001\r\n1072.303\r\n1881.667\r\ntau[2]\r\n2.615\r\n2.612\r\n0.122\r\n0.118\r\n2.419\r\n2.821\r\n1.005\r\n1547.561\r\n2254.383\r\nb_t[1]\r\n1.798\r\n1.794\r\n0.335\r\n0.335\r\n1.254\r\n2.349\r\n1.000\r\n6564.086\r\n3714.573\r\nb_t[2]\r\n1.699\r\n1.701\r\n0.310\r\n0.319\r\n1.183\r\n2.198\r\n1.000\r\n5728.963\r\n3821.598\r\nb_t[3]\r\n1.967\r\n1.971\r\n0.299\r\n0.298\r\n1.491\r\n2.462\r\n1.000\r\n6325.139\r\n3566.423\r\nb_t[4]\r\n2.302\r\n2.300\r\n0.319\r\n0.318\r\n1.783\r\n2.843\r\n1.001\r\n5648.104\r\n3809.811\r\nb_t[5]\r\n2.479\r\n2.488\r\n0.353\r\n0.344\r\n1.891\r\n3.061\r\n1.001\r\n6555.830\r\n3578.803\r\nb_t[6]\r\n1.969\r\n1.971\r\n0.326\r\n0.333\r\n1.424\r\n2.495\r\n1.000\r\n5906.941\r\n3710.413\r\nb_t[7]\r\n2.581\r\n2.577\r\n0.341\r\n0.330\r\n2.021\r\n3.159\r\n1.000\r\n5834.040\r\n3507.401\r\nb_t[8]\r\n3.065\r\n3.070\r\n0.351\r\n0.356\r\n2.490\r\n3.641\r\n1.001\r\n5946.883\r\n3773.475\r\nb_t[9]\r\n3.043\r\n3.039\r\n0.346\r\n0.336\r\n2.482\r\n3.622\r\n1.000\r\n6807.729\r\n2933.874\r\nb_t[10]\r\n2.824\r\n2.826\r\n0.348\r\n0.348\r\n2.242\r\n3.382\r\n1.000\r\n6918.646\r\n3719.018\r\nb_t[11]\r\n3.135\r\n3.140\r\n0.384\r\n0.381\r\n2.503\r\n3.762\r\n1.000\r\n7432.118\r\n3579.078\r\nb_t[12]\r\n3.175\r\n3.176\r\n0.383\r\n0.392\r\n2.540\r\n3.799\r\n1.001\r\n6264.057\r\n3811.178\r\nb_t[13]\r\n3.523\r\n3.525\r\n0.368\r\n0.370\r\n2.914\r\n4.133\r\n0.999\r\n6556.955\r\n3728.211\r\nb_t[14]\r\n3.838\r\n3.837\r\n0.398\r\n0.398\r\n3.180\r\n4.476\r\n0.999\r\n6121.546\r\n3726.720\r\nb_t[15]\r\n4.461\r\n4.459\r\n0.409\r\n0.406\r\n3.788\r\n5.148\r\n1.000\r\n6665.553\r\n3867.688\r\nb_t[16]\r\n5.957\r\n5.956\r\n0.403\r\n0.402\r\n5.299\r\n6.633\r\n1.000\r\n5896.254\r\n3454.660\r\nb_t[17]\r\n5.868\r\n5.873\r\n0.382\r\n0.391\r\n5.255\r\n6.487\r\n1.000\r\n5911.323\r\n3872.781\r\nb_t[18]\r\n6.461\r\n6.453\r\n0.394\r\n0.397\r\n5.827\r\n7.104\r\n1.001\r\n5171.089\r\n3228.712\r\nb_t[19]\r\n4.977\r\n4.976\r\n0.311\r\n0.312\r\n4.472\r\n5.495\r\n1.000\r\n5879.135\r\n3677.540\r\nb_t[20]\r\n4.169\r\n4.168\r\n0.265\r\n0.265\r\n3.724\r\n4.605\r\n1.000\r\n5316.392\r\n3666.933\r\nb_t[21]\r\n4.704\r\n4.693\r\n0.310\r\n0.307\r\n4.197\r\n5.218\r\n1.002\r\n6076.813\r\n3584.917\r\nb_t[22]\r\n5.870\r\n5.866\r\n0.358\r\n0.362\r\n5.289\r\n6.451\r\n1.000\r\n5298.588\r\n3418.853\r\nb_t[23]\r\n5.561\r\n5.566\r\n0.389\r\n0.392\r\n4.917\r\n6.199\r\n1.002\r\n6182.931\r\n3860.695\r\nb_t[24]\r\n4.562\r\n4.567\r\n0.441\r\n0.446\r\n3.835\r\n5.281\r\n1.000\r\n5299.736\r\n3824.800\r\nb_t[25]\r\n5.096\r\n5.094\r\n0.442\r\n0.442\r\n4.377\r\n5.835\r\n1.001\r\n5638.985\r\n3851.835\r\nb_t[26]\r\n5.659\r\n5.664\r\n0.444\r\n0.446\r\n4.944\r\n6.378\r\n1.000\r\n7011.250\r\n3646.906\r\nb_t[27]\r\n5.719\r\n5.716\r\n0.488\r\n0.479\r\n4.920\r\n6.516\r\n1.002\r\n6129.497\r\n3780.013\r\nb_t[28]\r\n6.448\r\n6.438\r\n0.612\r\n0.603\r\n5.446\r\n7.467\r\n1.001\r\n6038.999\r\n3444.120\r\nb_t[29]\r\n5.667\r\n5.654\r\n0.604\r\n0.610\r\n4.678\r\n6.646\r\n1.001\r\n5950.123\r\n3377.044\r\nb_t[30]\r\n4.365\r\n4.346\r\n0.601\r\n0.607\r\n3.415\r\n5.365\r\n1.000\r\n5965.089\r\n3543.557\r\nb_t[31]\r\n1.624\r\n1.622\r\n0.600\r\n0.591\r\n0.644\r\n2.604\r\n1.000\r\n4175.004\r\n3838.545\r\nrho\r\n0.894\r\n0.898\r\n0.037\r\n0.035\r\n0.828\r\n0.948\r\n1.000\r\n2739.444\r\n2701.058\r\nsig_t\r\n1.077\r\n1.057\r\n0.196\r\n0.185\r\n0.794\r\n1.430\r\n1.001\r\n1669.068\r\n2703.365\r\nL_Omega[2,1]\r\n0.133\r\n0.131\r\n0.093\r\n0.091\r\n-0.021\r\n0.292\r\n1.007\r\n382.611\r\n631.552\r\n\r\nSaving output for Panel E of Tab. 1\r\n\r\n\r\ntab1.E <- fit_wkinfo_timepers$summary(\r\n  variables = c(\"mu_b[1]\", \"sigma\", \"tau\", \"b_t\", \"rho\", \"sig_t\", \"L_Omega[2,1]\"), \r\n  mean, sd, ~quantile2(., probs = c(0.05, 0.25, 0.5, 0.75, 0.95))\r\n)\r\nwrite_csv(tab1.E, \"../out/results/tab1-panE.csv\")\r\n\r\n\r\n\r\n5.3 Comparing Bayesian ERC estimates to OLS estimates\r\n\r\n\r\nposterior_b2 <- summarise_draws(fit_wkinfo_timepers$draws(c(\"b_i\")), \r\n                               posterior_mean = mean, \r\n                               posterior_sd = sd,\r\n                               ~quantile2(., probs = c(0.05, 0.25, 0.75, 0.95))\r\n                               )\r\nposterior_erc2 <- \r\n  posterior_b2 |> \r\n  filter(str_detect(variable, \",2\\\\]\"))\r\n\r\nposterior_bt2 <- summarise_draws(fit_wkinfo_timepers$draws(c(\"b_t\")), \r\n                                 posterior_mean = mean, \r\n                                 posterior_sd = sd,\r\n                                 ~quantile2(., probs = c(0.05, 0.25, 0.75, 0.95))\r\n                                 )\r\n\r\nwrite_csv(posterior_bt2, \"../out/results/fit_wkinfo_time_bt.csv\")\r\nwrite_csv(posterior_b2, \"../out/results/fit_wkinfo_time_bi.csv\")\r\n\r\n\r\n\r\n\r\n\r\nfit_comparison2 <- \r\n  rbind(\r\n    desc_row(posterior_erc2$posterior_mean, \"post_mean\"),\r\n    desc_row(with(posterior_erc2, q95 - q5), \"post90_width\"),\r\n    desc_row(with(ols_results, estimate[is.na(estimate) == FALSE]), \"OLS\")\r\n  )\r\nwrite_csv(fit_comparison2, \"../out/results/fit_comp-AR1.csv\")\r\n\r\nfit_comparison2 |> \r\n  mutate(across(where(is.numeric), round, 3)) |> \r\n  kable()\r\n\r\n\r\n\r\nvar\r\nmean\r\nsd\r\nq5\r\nq25\r\nq50\r\nq75\r\nq95\r\n5%\r\npost_mean\r\n0.004\r\n1.273\r\n-2.213\r\n-0.548\r\n0.007\r\n0.598\r\n2.153\r\n5%1\r\npost90_width\r\n7.410\r\n1.259\r\n4.847\r\n6.664\r\n7.835\r\n8.413\r\n8.733\r\n5%2\r\nOLS\r\n6.041\r\n178.593\r\n-18.689\r\n0.068\r\n3.884\r\n10.650\r\n45.904\r\n\r\nCompare this to the numbers from the model without time trend\r\n\r\n\r\ntab1.C |>\r\n  mutate(across(where(is.numeric), round, 3)) |> \r\n  kable()\r\n\r\n\r\n\r\nvar\r\nmean\r\nsd\r\nq5\r\nq25\r\nq50\r\nq75\r\nq95\r\n5%\r\npost_mean\r\n3.672\r\n1.414\r\n1.292\r\n2.970\r\n3.676\r\n4.288\r\n6.105\r\n5%1\r\npost90_width\r\n7.837\r\n1.440\r\n4.946\r\n6.933\r\n8.314\r\n9.013\r\n9.345\r\n5%2\r\nOLS\r\n6.041\r\n178.593\r\n-18.689\r\n0.068\r\n3.884\r\n10.650\r\n45.904\r\n5%3\r\nOLS pval\r\n0.362\r\n0.309\r\n0.002\r\n0.068\r\n0.301\r\n0.611\r\n0.927\r\n\r\n5.4 Fig. 5\r\n\r\n\r\nposterior_bt2 |> \r\n  mutate(Year = 1:nrow(posterior_bt2) + 1989) |> \r\n  left_join(select(byyear_results, Year = EAYear, OLS = estimate),\r\n            by = c(\"Year\")) |>  \r\n  ggplot(aes(x = Year)) + \r\n  geom_ribbon(aes(ymin = q5, ymax = q95), alpha = 0.2) + \r\n  geom_ribbon(aes(ymin = q25, ymax = q75), alpha = 0.2) + \r\n  geom_line(aes(y = posterior_mean)) + \r\n  geom_line(aes(y = OLS), linetype = 2) +\r\n  scale_x_continuous(breaks = seq(1990, 2020, 5)) + \r\n  scale_y_continuous(limits = c(0, 8), expand = expansion(mult = c(0, 0))) + \r\n  labs(subtitle = expression(\"Yearly trend component\"~a1[t]~\"in ERCs\"),\r\n       y = \"Mean, 50%, and 90% interval\",\r\n       x = \"Calendar year of the announcement\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nf5.panA <-\r\n  posterior_bt2 |> \r\n  mutate(Year = 1:nrow(posterior_bt2) + 1989) |> \r\n  left_join(select(byyear_results, Year = EAYear, OLS = estimate),\r\n            by = c(\"Year\")) |>  \r\n  ggplot(aes(x = Year)) + \r\n  geom_ribbon(aes(ymin = q5, ymax = q95), alpha = 0.2) + \r\n  geom_ribbon(aes(ymin = q25, ymax = q75), alpha = 0.2) + \r\n  geom_line(aes(y = posterior_mean)) + \r\n  geom_line(aes(y = OLS), linetype = 2, color = \"grey40\") +\r\n  scale_x_continuous(breaks = seq(1990, 2020, 5)) + \r\n  scale_y_continuous(limits = c(0, 8), expand = expansion(mult = c(0, 0))) + \r\n  labs(subtitle = expression(\"Yearly trend component\"~a[1*\",\"*t]~\"in ERCs\"),\r\n       y = \"Mean, 50%, and 90% interval\",\r\n       x = \"Calendar year of the announcement\") + \r\n  annotate(\"text\", label = \"OLS estimates\", x = 2015.5, y = 3.3, size = 3, color = \"grey40\") + \r\n  annotate(\"text\", label = \"Bayes posterior\", x = 2004, y = 7.5, size = 3) \r\n\r\nf5.panB <-\r\n  posterior_erc2 |> \r\n  mutate(rank = rank(posterior_mean)) |> \r\n  ggplot(aes(x = rank, y = posterior_mean)) + \r\n  geom_hline(yintercept = 0) +\r\n  geom_linerange(aes(ymin = q5, ymax = q95), alpha = 0.1) +\r\n  geom_point(size = 0.5) + \r\n  scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) +\r\n  labs(y = \"Mean and 90% interval\",\r\n       x = expression(a[1*\",\"*i]~\"rank\"),\r\n       subtitle = expression(\"Firm-individual ERC component\"~a[1*\",\"*i]~\"from smallest to largest\"))\r\n\r\nfig5 <- \r\n  f5.panA / f5.panB + \r\n  plot_annotation(tag_levels = \"A\") &\r\n  theme(legend.position = \"none\")\r\nfig5\r\n\r\n\r\n\r\n\r\nSaving figure\r\n\r\n\r\nsave_fig(fig5, figname = \"fig5\", w = 6.2, h = 5)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-10-03T20:15:47+02:00"
    },
    {
      "path": "index.html",
      "title": "A Primer on Bayesian Inference for Accounting Research",
      "description": "Welcome to the companion website for the paper *What Can Bayesian Inference Do for Accounting Research?* I hope this site and the accompanying [GitHub repo](https://github.com/hschuett/BayesForAccountingResearch) will help those interested in Bayesian methods to find a starting point. Hopefully you find it useful.\n",
      "author": [
        {
          "name": "Harm Schuett",
          "url": "https://hschuett.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\nThe site is organized as follows. The first part provides the annotated code to produce the output and figures used in the paper (the GitHub repo also contains the code to compute the data used). Code can be copied easily to the clipboard and be reused. The second part is still under development and contains additional material useful to those interested in Bayesian analysis.\r\nPaper code\r\nSection 2: Simulation contains all the code to replicate the figures and results from Section 2, which compares Classical hypothesis testing to Bayesian inference.\r\nSection 3: ERC Example contains all the code to replicate the figures and results from the ERC example in Section 3. Only the prior predictive checks and cross-validation test are kept in the separate files (below).\r\nSection 3: ERC Prior Check contains the code to replicate the prior simulation check from the ERC example in Section 3.\r\nSection 3: ERC Prior Check contains the code to replicate the 10-fold cross validation test from the ERC example in Section 3.\r\nAdditional material\r\nMCMC primer is a very very brief explanation of how Bayesian models are fit using MCMC techniques. Definitely check out the links here that point towards more in-depth explanations.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-10-04T00:26:08+02:00"
    },
    {
      "path": "mcmc.html",
      "title": "A Brief Explanation of MCMC Sampling",
      "description": "A brief, high-level description of MCMC sampling methods used to fit Bayesian models.\n",
      "author": [
        {
          "name": "Harm Schuett",
          "url": "https://hschuett.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\n1. Introduction\r\n2. Markov Chain Monte Carlo Sampling\r\n3. Working with Markov Chains\r\n4. Conclusion\r\n\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(broom)\r\nlibrary(patchwork)\r\nlibrary(cmdstanr)\r\nsource(\"../R/00-utils.R\")\r\nkable <- knitr::kable\r\ntheme_set(theme_prodgray())\r\n\r\n\r\n\r\n1. Introduction\r\nConsider the simulation in Section 2, of the paper:\r\n\\[y = a_0 + a_1 * x + \\epsilon, \\, \\epsilon \\sim N(0, \\sigma)\\] \\[a_0 \\sim N(0, 100)\\] \\[a_1 \\sim N(0, 4)\\] \\[\\sigma \\sim Exponential(1/21)\\] This note explains the basics of how this model is fit to the simulated data and how draws from the posterior distribution are generated.\r\n\r\n\r\nShow code\r\n\r\nset.seed(888)\r\nn_samples <- 50\r\nn_obs <- 50\r\nx_fix <- rnorm(n = n_obs, 0, 1)\r\ngen_data <- function(n) tibble(u = rnorm(n, 0, 20), x = x_fix, y = 1 + 2 * x_fix + u)\r\n\r\n# gen samples\r\nsamples <- tibble(id = 1:n_samples)\r\nsamples$data <- map(rep.int(n_obs, n_samples), gen_data)\r\nsample1 <- samples$data[[1]]\r\ninput_data <- list(\r\n  y = sample1$y,\r\n  x = sample1$x,\r\n  N = nrow(sample1)\r\n)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nmodel_weak_priors <- cmdstan_model(\"../Stan/sim-wkinfo-priors.stan\")\r\nfit_weak_priors <- model_weak_priors$sample(\r\n  data = input_data,\r\n  iter_sampling = 1000,\r\n  iter_warmup = 1000,\r\n  chains = 4,\r\n  parallel_chains = 4,\r\n  seed = 1234,\r\n  save_warmup = TRUE,\r\n  refresh = 0\r\n)\r\n\r\n\r\nRunning MCMC with 4 parallel chains...\r\n\r\nChain 1 finished in 0.1 seconds.\r\nChain 3 finished in 0.1 seconds.\r\nChain 4 finished in 0.1 seconds.\r\nChain 2 finished in 0.1 seconds.\r\n\r\nAll 4 chains finished successfully.\r\nMean chain execution time: 0.1 seconds.\r\nTotal execution time: 0.4 seconds.\r\n\r\n2. Markov Chain Monte Carlo Sampling\r\nThe flexibility gained from working with probability distribution comes at the cost of computational complexity. Even moderately complex models have no analytically tractable posterior distribution. Instead, we use simulation methods to sample from the unknown posterior distribution. These methods, called Markov Chain Monte Carlo methods, draw “chains” of random samples from the unknown posterior (in our example, each sample draw is a triplet (\\(a_0\\),\\(a_1\\),\\(\\sigma\\))). A Markov chain is a sequence of random variables \\(\\theta_1\\), \\(\\theta_2\\), … (again, in our example, the triplet (\\(a_0\\),\\(a_1\\),\\(\\sigma\\))\\(_1\\), (\\(a_0\\),\\(a_1\\),\\(\\sigma\\))\\(_2\\),…) where the distribution of \\(\\theta_t\\) depends only on the most recent value \\(\\theta_{t-1}\\). MCMC algorithms do two things: 1) they draw samples of parameter values from an approximation to the unknown posterior and 2) with each draw, the approximation to the actual posterior distribution is improved. Once the chains run long enough to converge to the unknown posterior, the chains will draw (\\(a_0\\),\\(a_1\\),\\(\\sigma\\)) in proportion to the unknown posterior probability density of these values.\r\n\r\n\r\nShow code\r\n\r\npostdraws <- fit_weak_priors$draws(format = \"draws_df\", inc_warmup = TRUE)\r\n\r\ngraph_data <- as.data.frame(postdraws) |> \r\n  select(-lp__) |> \r\n  filter(.iteration < 1080) |> \r\n  pivot_longer(c(a0, a1, sigma), names_to = \"pars\")\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ngraph_data |>\r\n  mutate(pars2 = case_when(\r\n    pars == \"a0\" ~ \"a[0]\",\r\n    pars == \"a1\" ~ \"a[1]\",\r\n    pars == \"sigma\" ~ \"sigma\"\r\n  )) |> \r\n  ggplot(aes(x = .iteration, y = value, group = .chain, color = factor(.chain))) + \r\n  geom_line(alpha = 0.8, size = 0.5) + \r\n  facet_wrap(~pars2, ncol = 1, scales = \"free_y\", labeller = label_parsed) + \r\n  scale_x_continuous(expand = expansion(mult = c(0.02, 0))) +\r\n  scale_color_grey() + \r\n  labs(\r\n    x = \"Iteration\", \r\n    y = NULL\r\n    ) + \r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\nFigure 1: Warmup phase of four MCMC chains\r\n\r\n\r\n\r\nThe figure above shows the warmup phase (the first 1,000 iterations) of four Markov chains used to fit the model. Here the convergence happens almost immediately, and the chains traverse the same 3-dimensional space quickly. More likely values will be drawn more often, and less likely values less often. Thus, if the chains are run long enough, we will have empirical samples of the posterior. We can then compute anything we want to know about the posterior (mean, standard deviation, quantiles, etc.) basically by using the histogram of the samples (after discarding the convergence phase, shown below).\r\n\r\n\r\nShow code\r\n\r\nplot_range <- range(range(postdraws$a0), range(postdraws$a0))\r\n\r\nfig2.A <- \r\n  postdraws |>\r\n  filter(.iteration > 1000) |> \r\n  ggplot(aes(x = a0)) +\r\n  geom_histogram(binwidth = 0.3) +\r\n  labs(\r\n    x = expression(a[0]),\r\n    y = expression(\"Number of samples\")\r\n  ) +\r\n  scale_x_continuous(expand = c(0, 0), limits = plot_range) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.0, 0.05)))\r\n\r\nfig2.B <- \r\n  postdraws |>\r\n  filter(.iteration > 1000) |> \r\n  ggplot(aes(x = a1)) +\r\n  geom_histogram(binwidth = 0.3) +\r\n  labs(\r\n    x = expression(a[1]),\r\n    y = expression(\"Number of samples\")\r\n  ) +\r\n  scale_x_continuous(expand = c(0, 0), limits = plot_range) +\r\n  scale_y_continuous(expand = expansion(mult = c(0.0, 0.05)))\r\n\r\nfig2.A / fig2.B\r\n\r\n\r\n\r\n\r\nFigure 2: 4,000 MCMC samples from posterior\r\n\r\n\r\n\r\nThese histograms are used to reason about the posterior.\r\nThere are various algorithms for Markov chains (e.g., Metropolis-Hastings, Gibbs, Hamiltonian Monte Carlo), and developing new ones is an ongoing area of research. A detailed explanation of these algorithms is beyond the scope of this article, and interested readers are referred to Gelman et al. (2014). Here, I will instead focus on what is involved in fitting a Bayesian model using MCMC sampling.\r\n3. Working with Markov Chains\r\nImportant choices include how many chains to run in parallel and for how long. It is nearly always advisable to run several chains at once. The length of each chain (the number of iterations of draws) determines whether the chain has enough steps to converge to and then traverse the full posterior. A common starting point for modern MCMC algorithms is to run 4 chains with 2,000 iterations each, discarding the first 1,000 iterations of each chain. The remaining 4,000 iterations from all four chains serve as the empirical samples from the posterior from which we can compute posterior mean, standard deviation, and so on. Running several chains is done for three reasons.\r\nFirst, for a given set of iterations per chain, the more chains we run, the more samples we have, and thus the more detail about the empirical distribution is available to us. Aspects such as the tails of the posterior (e.g., the 1st or 99th percentile) are only precisely captured if we have many samples (i.e., 1,000+ effective samples).\r\nAlternatively, one could run one very long chain to have many samples. The second reason for running multiple chains, instead of one long chain, is that it helps assess when the chains have converged to the posterior. Markov chains need some iterations to converge to the posterior and start tracing it. This is often called “warm-up” or “burn-in” period and needs to be discarded before drawing any inferences from the samples. A common problem is to assess when such convergence has happened. With only one chain, one can only examine whether the chain is behaving erratically. However, with two or more chains, one can visually assess at which points the chains traverse the same parameter space in equal amounts. This is a sign that the chains have converged to the posterior. We thus need multiple chains to better assess convergence, which happens visually, via so-called trace plots (e.g., Figure 1) and using statistics, such as the so-called R-hat being close to 1 (Gelman and Rubin 1992).\r\nThird, and slightly related to point 2, in models with many parameters (e.g., the ERC model has 2,966 firm-level slope and intercept coefficients), the resulting high-dimensional posterior can have areas that are difficult for the Markov chains to traverse (see this case study by Michel Betancourt). Older algorithms, like the Metropolis sampler, are prone to getting “stuck” in narrow areas of the posterior for quite some time, which means we would need to run long chains to traverse the posterior properly. These are cases where the chains have converged, but cannot explore the posterior efficiently, which can lead to biased inference. If the chain is not run long enough, one chain might get lucky and not hit a complicated region in posterior space. Multiple chains, initialized in different random regions raise the probability of hitting complicated regions soon and thus help spotting issues without having to waste computing time on needlessly long chains. More recent MCMC implementations, such as those in Stan, are less prone to this issue and do a lot of auto-tuning. But for complex models, it can be necessary to either change the defaults or think about ways to reformulate the model to have a more well-behaved posterior. In my experience, such issues also often arise when the model (likelihood plus prior) does a poor job of reflecting the shape of the data, which is something that can be tested beforehand using prior predictive simulations.\r\n4. Conclusion\r\nFitting a Bayesian model using MCMC thus involves the additional steps of checking and sometimes tuning the Markov chains. For standard models (including standard multilevel models), the defaults nearly always work out of the box. For complex models, more work is required to get efficient chains. But the advantages of being able to fit fine-grained heterogeneity or complex latent processes is usually worth the effort.\r\n\r\n\r\n\r\nGelman, Andrew, John B. Carlin, Hal S. Stern, and Donald B. Rubin. 2014. Bayesian Data Analysis. Vol. 2. Chapman & Hall/CRC Boca Raton, FL, USA.\r\n\r\n\r\nGelman, Andrew, and Donald B. Rubin. 1992. “Inference from Iterative Simulation Using Multiple Sequences.” Statistical Science 7 (4): 457–72.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-10-04T00:30:46+02:00"
    }
  ],
  "collections": []
}
